# ğŸ”„ Models Execution Order - Visual Guide

**Quick Reference**: Execution order of models in the RAG system  
**Version**: v6.0  
**Date**: November 23, 2025

---

## ğŸ“‹ Quick Summary

```
SETUP PHASE (once per manual):
    Dolphin OCR â†’ E5-Large â†’ CLIP â†’ Databases ready

QUERY PHASE (every user question):
    E5-Large â†’ ChromaDB â†’ CLIP â†’ Gemini Flash â†’ Gemini Judge
```

---

## ğŸ—ï¸ PHASE 1: SETUP (BLOCK 0)

### Execution: Once per PDF manual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: PANDA.pdf manual (250 pages)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  MODEL #1: DOLPHIN OCR                â”ƒ
        â”ƒ  (GOT-OCR2_0)                         â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  What it does:                        â”ƒ
        â”ƒ  â€¢ Reads each PDF page                â”ƒ
        â”ƒ  â€¢ Extracts text (OCR)                â”ƒ
        â”ƒ  â€¢ Identifies and crops images        â”ƒ
        â”ƒ  â€¢ Maintains document structure       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Parameters:                          â”ƒ
        â”ƒ  â€¢ batch_size: 1 page at a time       â”ƒ
        â”ƒ  â€¢ device: CUDA (GPU)                 â”ƒ
        â”ƒ  â€¢ dpi: 300 (high resolution)         â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: ~5 minutes (250 pages)         â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  OUTPUT:                            â”‚
        â”‚  â€¢ 2,541 text chunks                â”‚
        â”‚  â€¢ 850 PNG images                   â”‚
        â”‚  â€¢ Metadata (page, bbox, etc.)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â†“                   â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ MODEL #2:          â”ƒ   â”ƒ MODEL #3:          â”ƒ
â”ƒ E5-LARGE           â”ƒ   â”ƒ CLIP               â”ƒ
â”ƒ (Text Embedder)    â”ƒ   â”ƒ (Image Embedder)   â”ƒ
â”ƒ                    â”ƒ   â”ƒ                    â”ƒ
â”ƒ What it does:      â”ƒ   â”ƒ What it does:      â”ƒ
â”ƒ â€¢ Converts text    â”ƒ   â”ƒ â€¢ Converts images  â”ƒ
â”ƒ   to vectors       â”ƒ   â”ƒ   to vectors       â”ƒ
â”ƒ â€¢ 1024 dimensions  â”ƒ   â”ƒ â€¢ 512 dimensions   â”ƒ
â”ƒ â€¢ L2 normalization â”ƒ   â”ƒ â€¢ L2 normalization â”ƒ
â”ƒ                    â”ƒ   â”ƒ                    â”ƒ
â”ƒ Input:             â”ƒ   â”ƒ Input:             â”ƒ
â”ƒ â€¢ 2,541 chunks     â”ƒ   â”ƒ â€¢ 850 images       â”ƒ
â”ƒ                    â”ƒ   â”ƒ â€¢ Quality filteringâ”ƒ
â”ƒ Parameters:        â”ƒ   â”ƒ                    â”ƒ
â”ƒ â€¢ batch_size: 32   â”ƒ   â”ƒ Parameters:        â”ƒ
â”ƒ â€¢ device: cuda     â”ƒ   â”ƒ â€¢ size: 224Ã—224    â”ƒ
â”ƒ â€¢ fp16: True       â”ƒ   â”ƒ â€¢ device: cuda     â”ƒ
â”ƒ                    â”ƒ   â”ƒ â€¢ fp16: True       â”ƒ
â”ƒ Time: ~2 min       â”ƒ   â”ƒ                    â”ƒ
â”ƒ   (all chunks)     â”ƒ   â”ƒ Time: ~3 min       â”ƒ
â”ƒ                    â”ƒ   â”ƒ   (595 quality)    â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›   â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
        â†“                          â†“
        â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB (Text)  â”‚   â”‚ ChromaDB (Images) â”‚
â”‚  2,541 vectors    â”‚   â”‚ 595 vectors       â”‚
â”‚  [1024-dim]       â”‚   â”‚ [512-dim]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… SETUP COMPLETE                                           â”‚
â”‚  Databases ready for queries                                â”‚
â”‚  Total time: ~10 minutes                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” PHASE 2: QUERY (BLOCK 3)

### Execution: Every user question

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: "How does ASR work on PANDA?"                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  STEP 1: PREPROCESSING                â”ƒ
        â”ƒ  (Not an AI model)                    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  â€¢ PromptProtection: sanitize query   â”ƒ
        â”ƒ  â€¢ VehicleDetector: find "PANDA"     â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: <1ms                           â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  MODEL #2: E5-LARGE (REUSED)          â”ƒ
        â”ƒ  Text Embedding                       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  What it does:                        â”ƒ
        â”ƒ  â€¢ Converts query to vector           â”ƒ
        â”ƒ  â€¢ Same model used for indexing       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Input: "How does ASR work on PANDA?" â”ƒ
        â”ƒ  Output: vector [1024 dimensions]    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: 22ms                           â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  CHROMADB TEXT SEARCH               â”‚
        â”‚  (Not an AI model)                  â”‚
        â”‚                                     â”‚
        â”‚  â€¢ Calculate cosine similarity      â”‚
        â”‚  â€¢ Query vs all 2,541 chunks        â”‚
        â”‚  â€¢ Filter by manual="PANDA"         â”‚
        â”‚  â€¢ Sort by similarity               â”‚
        â”‚  â€¢ Return top 30                    â”‚
        â”‚                                     â”‚
        â”‚  Time: 6ms                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STAGE 1 COMPLETE                   â”‚
        â”‚  Top 30 text chunk candidates       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  MODEL #3: CLIP (REUSED)              â”ƒ
        â”ƒ  Visual Re-ranking                    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  What it does:                        â”ƒ
        â”ƒ  â€¢ Encode query as text (CLIP)        â”ƒ
        â”ƒ  â€¢ Find images in pages of            â”ƒ
        â”ƒ    the 30 chunks                      â”ƒ
        â”ƒ  â€¢ Calculate visual similarity        â”ƒ
        â”ƒ  â€¢ Combine with text similarity       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Hybrid Score Formula:                â”ƒ
        â”ƒ  score = 0.55Ã—CLIP + 0.45Ã—text       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Input:                               â”ƒ
        â”ƒ  â€¢ Query text: "How does ASR..."      â”ƒ
        â”ƒ  â€¢ ~18 candidate images               â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Output: Top 6 images + chunks        â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: 105ms                          â”ƒ
        â”ƒ  â€¢ Text encode: 8ms                   â”ƒ
        â”ƒ  â€¢ Image similarity: 97ms             â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  QUALITY FILTER                     â”‚
        â”‚  (Not an AI model)                  â”‚
        â”‚                                     â”‚
        â”‚  Checks:                            â”‚
        â”‚  â€¢ File size â‰¥ 10KB                 â”‚
        â”‚  â€¢ Resolution â‰¥ 150Ã—150px           â”‚
        â”‚  â€¢ Aspect ratio â‰¤ 6.0               â”‚
        â”‚                                     â”‚
        â”‚  Input: 6 images                    â”‚
        â”‚  Output: 4 images pass âœ…           â”‚
        â”‚                                     â”‚
        â”‚  Time: 3ms                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STAGE 2 COMPLETE                   â”‚
        â”‚  â€¢ 6 final text chunks              â”‚
        â”‚  â€¢ 4 quality images                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  MODEL #4: GEMINI 2.0 FLASH LITE      â”ƒ
        â”ƒ  RAG Generation                       â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  What it does:                        â”ƒ
        â”ƒ  â€¢ Receives 6 chunks as context       â”ƒ
        â”ƒ  â€¢ Receives user query                â”ƒ
        â”ƒ  â€¢ Generates natural language         â”ƒ
        â”ƒ    response                           â”ƒ
        â”ƒ  â€¢ Grounded response (no hallucination)â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Input:                               â”ƒ
        â”ƒ  â€¢ System prompt: "Answer based on..."â”ƒ
        â”ƒ  â€¢ Context: [6 chunks, ~1,500 tokens] â”ƒ
        â”ƒ  â€¢ Question: "How does ASR work..."   â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Parameters:                          â”ƒ
        â”ƒ  â€¢ temperature: 0.1 (deterministic)   â”ƒ
        â”ƒ  â€¢ max_tokens: 512                    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Output:                              â”ƒ
        â”ƒ  "The ASR system prevents wheel       â”ƒ
        â”ƒ   spin during acceleration..."        â”ƒ
        â”ƒ  (~85 tokens)                         â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: 1,050ms                        â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  CONFIDENCE CALCULATION             â”‚
        â”‚  (Not an AI model)                  â”‚
        â”‚                                     â”‚
        â”‚  Multi-source confidence:           â”‚
        â”‚  â€¢ Retrieval quality: 0.91          â”‚
        â”‚  â€¢ Context relevance: 0.84          â”‚
        â”‚  â€¢ Answer quality: 0.70             â”‚
        â”‚                                     â”‚
        â”‚  Aggregate: 0.83 (HIGH) âœ…          â”‚
        â”‚                                     â”‚
        â”‚  Time: 5ms                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  MODEL #5: GEMINI 2.0                 â”ƒ
        â”ƒ  LLM Judge (OPTIONAL)                 â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  What it does:                        â”ƒ
        â”ƒ  â€¢ Evaluates response quality         â”ƒ
        â”ƒ  â€¢ 3 dimensions:                      â”ƒ
        â”ƒ    1. Faithfulness (context grounding)â”ƒ
        â”ƒ    2. Relevance (query addressing)    â”ƒ
        â”ƒ    3. Completeness (comprehensive)    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Input:                               â”ƒ
        â”ƒ  â€¢ Original query                     â”ƒ
        â”ƒ  â€¢ Context (6 chunks)                 â”ƒ
        â”ƒ  â€¢ Generated response                 â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Output (JSON):                       â”ƒ
        â”ƒ  {                                    â”ƒ
        â”ƒ    "faithfulness": 5,                 â”ƒ
        â”ƒ    "relevance": 4,                    â”ƒ
        â”ƒ    "completeness": 4,                 â”ƒ
        â”ƒ    "average": 4.33                    â”ƒ
        â”ƒ  }                                    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Parameters:                          â”ƒ
        â”ƒ  â€¢ temperature: 0.0 (deterministic)   â”ƒ
        â”ƒ  â€¢ max_tokens: 500                    â”ƒ
        â”ƒ                                       â”ƒ
        â”ƒ  Time: 2,600ms                        â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  FINAL RESPONSE PACKAGE             â”‚
        â”‚                                     â”‚
        â”‚  â€¢ Answer text                      â”‚
        â”‚  â€¢ 4 quality images                 â”‚
        â”‚  â€¢ Confidence: 0.83 (HIGH)          â”‚
        â”‚  â€¢ Judge scores: 4.33/5             â”‚
        â”‚  â€¢ Complete metrics                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… QUERY COMPLETE                                           â”‚
â”‚  Total time: 3.8s (with judge), 1.2s (without judge)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Visual Timeline

```
SETUP PHASE (once):
â”‚
â”œâ”€â–º [0-5min]   Dolphin OCR: PDF â†’ Text + Images
â”œâ”€â–º [5-7min]   E5-Large: Text â†’ Embeddings (1024-dim)
â”œâ”€â–º [7-10min]  CLIP: Images â†’ Embeddings (512-dim)
â””â”€â–º [10min]    âœ… Databases ready

QUERY PHASE (every request):
â”‚
â”œâ”€â–º [0ms]      Preprocessing (VehicleDetector)
â”œâ”€â–º [22ms]     E5-Large: Query â†’ Embedding
â”œâ”€â–º [28ms]     ChromaDB: Text search (top 30)
â”œâ”€â–º [133ms]    CLIP: Visual re-ranking (top 6)
â”œâ”€â–º [136ms]    Quality filter (4 images pass)
â”œâ”€â–º [1,186ms]  Gemini Flash: Generate answer
â”œâ”€â–º [1,191ms]  ConfidenceCalculator
â”œâ”€â–º [3,791ms]  Gemini Judge: Evaluate (optional)
â””â”€â–º [3,791ms]  âœ… Response ready

Average time without judge: 1.2s
Average time with judge: 3.8s
```

---

## ğŸ”¢ Numerical Order

### Setup Phase
1. **Dolphin OCR** (GOT-OCR2_0)
2. **E5-Large** (multilingual-e5-large)
3. **CLIP** (openai/clip-vit-base-patch32)

### Query Phase (Runtime)
1. **Preprocessing** (VehicleDetector, PromptProtection)
2. **E5-Large** (REUSED: query embedding)
3. **ChromaDB** (similarity search)
4. **CLIP** (REUSED: visual re-ranking)
5. **ImageQualityFilter** (quality check)
6. **Gemini Flash Lite** (answer generation)
7. **ConfidenceCalculator** (aggregate metrics)
8. **Gemini 2.0** (OPTIONAL: LLM Judge)

---

## ğŸ¯ Key Points for Presentation

### Quick Explanation (1 minute)

**Setup Phase**:
> "Before we can answer questions, we prepare the database using 3 models in sequence: **Dolphin OCR** reads the 250-page PDF and extracts 2,541 text blocks and 850 images. Then **E5-Large** converts all text into 1024-dimensional vectors representing semantic meaning. Finally, **CLIP** converts images into 512-dimensional vectors. Everything is stored in ChromaDB vector databases. This takes about 10 minutes but only happens once per manual."

**Query Phase**:
> "When a user asks a question, 4 main steps occur:
> 
> 1. **E5-Large** (reused) converts the question into a vector and finds the 30 most semantically similar text chunks in the database (28ms)
> 
> 2. **CLIP** (reused) analyzes images in candidate pages and re-ranks them by combining text similarity (45%) and visual similarity (55%), selecting the top 6 results (105ms)
> 
> 3. **Gemini Flash Lite** generates a natural language answer based only on the 6 retrieved chunks, avoiding hallucinations (1050ms)
> 
> 4. Optionally, **Gemini 2.0** evaluates the quality on 3 criteria: faithfulness to context, relevance to question, and completeness (2600ms)
> 
> Total time: 1.2 seconds without evaluation, or 3.8 seconds with complete evaluation."

### Detailed Explanation (3 minutes)

**Setup Phase (10 minutes total)**:
> "The setup phase uses 3 AI models to transform a raw PDF into a searchable database. First, **Dolphin OCR** with the GOT-OCR2_0 architecture processes each page using a Vision Transformer encoder. It identifies text regions, performs OCR with 98%+ accuracy on automotive technical content, and extracts images with their bounding boxes and surrounding context. This produces 2,541 text chunks and 850 PNG images.
>
> Next, **E5-Large** (multilingual-e5-large), a 560M parameter sentence transformer, embeds all text chunks. It uses a 24-layer XLM-RoBERTa encoder to create 1024-dimensional dense vectors that capture semantic meaning. These vectors are L2-normalized and stored in ChromaDB for efficient similarity search.
>
> Finally, **CLIP ViT-B/32**, a 151M parameter dual-encoder model, processes the images. After quality filtering (resolution â‰¥150Ã—150px, size â‰¥10KB, aspect ratio â‰¤6.0), 595 high-quality images are embedded into 512-dimensional vectors in the same semantic space as text. This allows us to search both text and images using natural language queries."

**Query Phase (1.2-3.8 seconds)**:
> "When a user asks a question, the system executes a sophisticated two-stage retrieval process:
>
> **Stage 1 - Text Retrieval (28ms)**: E5-Large embeds the query using the same model that indexed the documents, ensuring consistency. ChromaDB then performs a cosine similarity search across all 2,541 chunks, filtering by detected vehicle if applicable. The top 30 candidates are selected based on semantic similarity.
>
> **Stage 2 - Visual Re-ranking (105ms)**: CLIP encodes the query text through its text encoder, producing a 512-dimensional vector. For each image in the candidate pages, CLIP computes visual similarity. A hybrid score combines text similarity (45%) and visual similarity (55%) - this weight was empirically optimized on a 50-query validation set. The top 6 results balance both semantic and visual relevance.
>
> After quality filtering removes low-resolution or oddly-shaped images, **Gemini 2.0 Flash Lite** generates the answer. Using a carefully crafted RAG prompt with low temperature (0.1) for deterministic output, it produces a grounded response in about 1 second. The model attends to the retrieved context and avoids hallucinations by explicitly instructing it to use only provided information.
>
> Finally, an optional **LLM-as-a-Judge** evaluation uses Gemini 2.0 to score the response on faithfulness (grounding in context), relevance (addressing the query), and completeness (comprehensive answer). This methodology, based on recent NeurIPS 2023 research, achieves 86% agreement with human expert evaluations."

### Why This Architecture?

âœ… **Multi-modal**: CLIP unites text and images in the same space  
âœ… **Two-stage**: Fast text search (E5) refined by precise visual ranking (CLIP)  
âœ… **Grounded**: Gemini Flash uses ONLY retrieved context  
âœ… **Evaluated**: Gemini Judge provides objective metrics (86% human agreement)  
âœ… **SOTA 2025**: State-of-the-art techniques validated by research

### Results

- **Image F1**: 0.89 (CLIP re-ranking excellence)
- **Text F1**: 0.75 (E5-Large retrieval)
- **Confidence**: 0.81 average (68% HIGH)
- **LLM Judge**: 4.37/5 average
- **User Satisfaction**: 4.6/5 (+64% from v0_whatsapp)
- **Response Time**: 1.2s (production-ready)

---

## ğŸ“ Simplified Nomenclature

To make the presentation clearer, you can use these simplified names:

| Technical Name | Simple Name | Function |
|----------------|-------------|----------|
| Dolphin OCR (GOT-OCR2_0) | "PDF Reader" | Extracts text and images |
| multilingual-e5-large | "Text Embedder" | Converts text to numbers |
| CLIP ViT-B/32 | "Visual Embedder" | Converts images to numbers |
| Gemini 2.0 Flash Lite | "Answer Generator" | Creates the final response |
| Gemini 2.0 | "Quality Judge" | Evaluates responses |

---

## ğŸ”„ Complete Flow Diagram

```
SETUP (Once)                    QUERY (Every time)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PDF Manual                      User Question
    â†“                               â†“
[Dolphin OCR]                   Preprocessing
    â†“                               â†“
Text + Images                   [E5-Large]
    â†“                               â†“
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”                      Query Vector
â†“        â†“                           â†“
[E5]   [CLIP]                   ChromaDB Search
â†“        â†“                           â†“
Text   Image                    Top 30 Chunks
DB     DB                            â†“
                                [CLIP Re-rank]
                                    â†“
                                Top 6 + Images
                                    â†“
                                Quality Filter
                                    â†“
                                4-6 Images
                                    â†“
                                [Gemini Flash]
                                    â†“
                                Answer
                                    â†“
                                Confidence
                                    â†“
                                [Gemini Judge]
                                    â†“
                                Complete Response
```

---

## ğŸ’¡ Teaching Tips

### For Technical Audience

Focus on:
- **Architecture decisions**: Why two-stage? Why CLIP weight 0.55?
- **Empirical validation**: 50-query validation set, 100-image threshold analysis
- **Performance optimization**: FP16, batch processing, GPU memory management
- **Evaluation methodology**: LLM-as-Judge, multi-source confidence

### For Non-Technical Audience

Use analogies:
- **E5-Large**: "Like a librarian who remembers where every topic is discussed"
- **CLIP**: "Like someone who can look at a picture and understand what it shows"
- **Gemini Flash**: "Like an expert who reads the manual and explains it to you"
- **Gemini Judge**: "Like a teacher grading homework on accuracy and completeness"

### Common Questions

**Q: Why not use just one big model?**
A: Specialized models excel at specific tasks. CLIP is best at visual-semantic matching, E5-Large is best at text similarity, Gemini is best at natural language generation. Combining them gives better results than any single model.

**Q: Why 1024 dimensions for text but only 512 for images?**
A: Text has more semantic nuance requiring higher dimensionality. Images can be effectively represented in 512 dimensions for matching purposes. These dimensions were chosen by the model creators based on extensive research.

**Q: How do you avoid hallucinations?**
A: By using RAG (Retrieval-Augmented Generation) with strict prompting. We explicitly instruct Gemini to use ONLY the retrieved context and to say "I don't know" if information isn't present. The low temperature (0.1) also makes output more deterministic and faithful.

**Q: Why use Gemini instead of open-source models?**
A: Gemini Flash Lite offers the best balance of speed (<1s), quality (near GPT-4), and cost (10x cheaper than full Gemini). For production deployment serving many users, this combination is optimal. Open-source alternatives would require expensive GPU infrastructure.

---

**Document Version**: 2.0  
**Last Updated**: November 23, 2025  
**Language**: English  
**Companion Document**: MODELS_DEEP_DIVE.md
