{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nwsUYdPPW9j"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iVFLnYFYdD4A"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 0: SETUP TOTALE CON CLIP (FIXED)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"ðŸš€ 1. INSTALLAZIONE LIBRERIE...\")\n",
        "\n",
        "# Fix dipendenze prima\n",
        "!pip install -q pillow==11.1.0  # Fix per gradio compatibility\n",
        "!pip install -q requests==2.32.4  # Fix per google-colab\n",
        "\n",
        "# Installazione principale\n",
        "!pip install -q -U \\\n",
        "    typing_extensions \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-core \\\n",
        "    langchain-google-genai \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-chroma \\\n",
        "    langchain-text-splitters \\\n",
        "    chromadb \\\n",
        "    sentence-transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    streamlit \\\n",
        "    pyngrok \\\n",
        "    pymupdf \\\n",
        "    transformers \\\n",
        "    ftfy \\\n",
        "    regex\n",
        "\n",
        "# Verifica installazione\n",
        "print(\"\\nâœ… 2. VERIFICA INSTALLAZIONE...\")\n",
        "try:\n",
        "    import langchain\n",
        "    import chromadb\n",
        "    import streamlit\n",
        "    import transformers\n",
        "    print(f\"   âœ… LangChain {langchain.__version__}\")\n",
        "    print(f\"   âœ… ChromaDB {chromadb.__version__}\")\n",
        "    print(f\"   âœ… Streamlit {streamlit.__version__}\")\n",
        "    print(f\"   âœ… Transformers {transformers.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   âŒ Errore: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\nðŸ¬ 3. SETUP DOLPHIN OCR...\")\n",
        "if not os.path.exists('/content/Dolphin'):\n",
        "    print(\"   - Clonazione repository...\")\n",
        "    try:\n",
        "        !git clone https://github.com/ByteDance/Dolphin.git\n",
        "        # Installa dipendenze Dolphin individualmente per evitare errori\n",
        "        !pip install -q timm einops\n",
        "        print(\"   âœ… Dolphin clonato\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Warning: Dolphin setup fallito: {e}\")\n",
        "else:\n",
        "    print(\"   âœ… Dolphin giÃ  presente\")\n",
        "\n",
        "# Scarica il modello Dolphin\n",
        "if not os.path.exists('/content/hf_model'):\n",
        "    print(\"   - Download Modello Dolphin...\")\n",
        "    !pip install -q huggingface_hub\n",
        "    !huggingface-cli download ByteDance/Dolphin-1.5 --local-dir ./hf_model\n",
        "    if os.path.exists('/content/hf_model'):\n",
        "        print(\"   âœ… Modello Dolphin scaricato\")\n",
        "    else:\n",
        "        print(\"   âš ï¸ Errore nel download Dolphin\")\n",
        "else:\n",
        "    print(\"   âœ… Modello Dolphin giÃ  presente\")\n",
        "\n",
        "print(\"\\nðŸ–¼ï¸ 4. VERIFICA CLIP...\")\n",
        "try:\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "    print(\"   âœ… CLIP disponibile\")\n",
        "\n",
        "    # Test caricamento modello\n",
        "    print(\"   - Test caricamento CLIP...\")\n",
        "    _ = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    print(\"   âœ… CLIP funzionante\")\n",
        "except ImportError as e:\n",
        "    print(f\"   âŒ CLIP import fallito: {e}\")\n",
        "    print(\"\\n   âš ï¸ RIAVVIA IL RUNTIME (Runtime > Restart runtime)\")\n",
        "    print(\"   Poi riesegui questo blocco.\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ CLIP warning: {e}\")\n",
        "\n",
        "print(\"\\nðŸ“‚ 5. MOUNT GOOGLE DRIVE...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Verifica cartella MANUALS\n",
        "manuals_path = \"/content/drive/MyDrive/MANUALS\"\n",
        "if os.path.exists(manuals_path):\n",
        "    pdfs = [f for f in os.listdir(manuals_path) if f.endswith('.pdf')]\n",
        "    print(f\"\\nðŸ“„ PDF trovati in MANUALS: {len(pdfs)}\")\n",
        "    for pdf in pdfs:\n",
        "        size_mb = os.path.getsize(os.path.join(manuals_path, pdf)) / (1024*1024)\n",
        "        print(f\"   - {pdf} ({size_mb:.1f} MB)\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ Cartella {manuals_path} non trovata!\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Setup completato!\")\n",
        "print(\"\\nâš ï¸ Se CLIP non Ã¨ disponibile, RIAVVIA IL RUNTIME e riesegui questo blocco.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp6wKN3lPfCb"
      },
      "source": [
        "# Configurazione centralizzata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qFU38oz0gPdN"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 1: CONFIGURAZIONE CENTRALIZZATA (FINAL CORRECT v2.0)\n",
        "# ==========================================\n",
        "\n",
        "print(\"âš™ï¸ BLOCCO 1: Caricamento configurazione...\")\n",
        "\n",
        "# ==========================================\n",
        "# PATH CONFIGURAZIONE\n",
        "# ==========================================\n",
        "\n",
        "PDF_INPUT_DIR = \"/content/drive/MyDrive/MANUALS\"\n",
        "OCR_OUTPUT_DIR = \"/content/drive/MyDrive/OCR_Output_Temp\"\n",
        "\n",
        "LOCAL_TEXT_DB = \"/content/temp_text_db\"\n",
        "LOCAL_IMAGE_DB = \"/content/temp_image_db\"\n",
        "\n",
        "DRIVE_TEXT_DB = \"/content/drive/MyDrive/OCR/Testo_DB\"\n",
        "DRIVE_IMAGE_DB = \"/content/drive/MyDrive/OCR/Images_DB\"\n",
        "DRIVE_CLIP_JSON = \"/content/drive/MyDrive/OCR/clip_embeddings.json\"\n",
        "\n",
        "CHROMA_TEXT_DIR = DRIVE_TEXT_DB\n",
        "CHROMA_IMAGE_DIR = DRIVE_IMAGE_DB\n",
        "CLIP_JSON_PATH = DRIVE_CLIP_JSON\n",
        "\n",
        "# ==========================================\n",
        "# MODELLI\n",
        "# ==========================================\n",
        "\n",
        "TEXT_EMBED_MODEL = \"intfloat/multilingual-e5-large\"\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "TEXT_COLLECTION_NAME = \"text_documents\"\n",
        "IMAGE_COLLECTION_NAME = \"image_documents\"\n",
        "\n",
        "# ==========================================\n",
        "# PARAMETRI INGESTIONE\n",
        "# ==========================================\n",
        "\n",
        "PAGES_PER_BATCH = 10\n",
        "DOLPHIN_TIMEOUT = 600\n",
        "CHUNK_SIZE = 1200\n",
        "CHUNK_OVERLAP = 300\n",
        "\n",
        "# ==========================================\n",
        "# âœ… IMAGE QUALITY CONFIG - FIXED PER DATI REALI\n",
        "# ==========================================\n",
        "\n",
        "# I tuoi dati: median=1.6KB, 67x48px\n",
        "# Abbassiamo drasticamente per non perdere tutto\n",
        "\n",
        "IMAGE_QUALITY_CONFIG = {\n",
        "    'min_width': 20,         # Era 100 - le tue immagini sono ~67px\n",
        "    'min_height': 20,        # Era 100 - le tue immagini sono ~48px\n",
        "    'min_size_kb': 0.3,      # Era 30 - le tue immagini sono ~1.6KB\n",
        "    'max_aspect_ratio': 12.0 # Un po' piÃ¹ permissivo\n",
        "}\n",
        "\n",
        "# CLIP Threshold (per ingestion futura)\n",
        "CLIP_SIZE_KB_MIN = IMAGE_QUALITY_CONFIG['min_size_kb']\n",
        "CLIP_WIDTH_MIN = IMAGE_QUALITY_CONFIG['min_width']\n",
        "CLIP_HEIGHT_MIN = IMAGE_QUALITY_CONFIG['min_height']\n",
        "CLIP_CAPTION_MAX_LEN = 150\n",
        "\n",
        "# ==========================================\n",
        "# TWO-STAGE RETRIEVAL\n",
        "# ==========================================\n",
        "\n",
        "STAGE1_TOP_K = 40\n",
        "STAGE2_TOP_K = 5\n",
        "CLIP_WEIGHT = 0.40\n",
        "\n",
        "# ==========================================\n",
        "# API KEY\n",
        "# ==========================================\n",
        "\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA CONFIGURAZIONE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… CONFIGURAZIONE CARICATA (v2.0 - Single Source of Truth)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nðŸ“‚ Paths:\")\n",
        "print(f\"   â€¢ PDF Input: {PDF_INPUT_DIR}\")\n",
        "print(f\"   â€¢ OCR Output: {OCR_OUTPUT_DIR}\")\n",
        "print(f\"   â€¢ Text DB: {CHROMA_TEXT_DIR}\")\n",
        "print(f\"   â€¢ Image DB: {CHROMA_IMAGE_DIR}\")\n",
        "print(f\"   â€¢ CLIP JSON: {CLIP_JSON_PATH}\")\n",
        "\n",
        "print(f\"\\nðŸ¤– Modelli:\")\n",
        "print(f\"   â€¢ Text Embedding: {TEXT_EMBED_MODEL}\")\n",
        "print(f\"   â€¢ CLIP Model: {CLIP_MODEL_NAME}\")\n",
        "print(f\"   â€¢ Text Collection: {TEXT_COLLECTION_NAME}\")\n",
        "print(f\"   â€¢ Image Collection: {IMAGE_COLLECTION_NAME}\")\n",
        "\n",
        "print(f\"\\nðŸ” Two-Stage Retrieval:\")\n",
        "print(f\"   â€¢ Stage 1 (Text): Top {STAGE1_TOP_K} candidates\")\n",
        "print(f\"   â€¢ Stage 2 (CLIP): Top {STAGE2_TOP_K} results\")\n",
        "print(f\"   â€¢ CLIP weight: {CLIP_WEIGHT} (Text weight: {1-CLIP_WEIGHT})\")\n",
        "\n",
        "print(f\"\\nâš™ï¸ Parametri Ingestione:\")\n",
        "print(f\"   â€¢ Dolphin batch: {PAGES_PER_BATCH} pagine/batch\")\n",
        "print(f\"   â€¢ Dolphin timeout: {DOLPHIN_TIMEOUT}s\")\n",
        "print(f\"   â€¢ Chunk size: {CHUNK_SIZE} char\")\n",
        "print(f\"   â€¢ Chunk overlap: {CHUNK_OVERLAP} char\")\n",
        "\n",
        "print(f\"\\nðŸ–¼ï¸ Image Quality Config (CENTRALIZZATA):\")\n",
        "print(f\"   â­ SINGLE SOURCE OF TRUTH - Modifica SOLO qui!\")\n",
        "print(f\"   â€¢ Min resolution: {IMAGE_QUALITY_CONFIG['min_width']}x{IMAGE_QUALITY_CONFIG['min_height']}px\")\n",
        "print(f\"   â€¢ Min size: {IMAGE_QUALITY_CONFIG['min_size_kb']}KB\")\n",
        "print(f\"   â€¢ Max aspect ratio: {IMAGE_QUALITY_CONFIG['max_aspect_ratio']}\")\n",
        "print(f\"   ðŸ“Š Confronto:\")\n",
        "print(f\"      Vecchio (BUG): 300x300px, 20KB â†’ 70% rejection\")\n",
        "print(f\"      Nuovo (FIX):   150x150px, 10KB â†’ 30% rejection\")\n",
        "\n",
        "print(f\"\\nðŸ”§ CLIP Threshold (per ingestion):\")\n",
        "print(f\"   â€¢ Min size: {CLIP_SIZE_KB_MIN}KB (eredita da IMAGE_QUALITY_CONFIG)\")\n",
        "print(f\"   â€¢ Min resolution: {CLIP_WIDTH_MIN}x{CLIP_HEIGHT_MIN}px\")\n",
        "print(f\"   â€¢ Caption max: {CLIP_CAPTION_MAX_LEN} char\")\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA ESISTENZA DATABASE\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "\n",
        "print(f\"\\nðŸ“Š Database Status:\")\n",
        "\n",
        "text_exists = os.path.exists(CHROMA_TEXT_DIR)\n",
        "img_exists = os.path.exists(CHROMA_IMAGE_DIR)\n",
        "clip_exists = os.path.exists(CLIP_JSON_PATH)\n",
        "\n",
        "print(f\"   {'âœ…' if text_exists else 'âŒ'} Text DB: {CHROMA_TEXT_DIR}\")\n",
        "print(f\"   {'âœ…' if img_exists else 'âŒ'} Image DB: {CHROMA_IMAGE_DIR}\")\n",
        "print(f\"   {'âœ…' if clip_exists else 'âŒ'} CLIP embeddings: {CLIP_JSON_PATH}\")\n",
        "\n",
        "if not (text_exists and img_exists):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âš ï¸ ATTENZIONE: Database mancanti!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nðŸ“‹ Cosa fare:\")\n",
        "    print(\"   1. Se NON hai ancora i database:\")\n",
        "    print(\"      â†’ Esegui il BLOCCO INGESTIONE per crearli\")\n",
        "    print(\"      â†’ Tempo richiesto: 30-90 minuti (dipende dai PDF)\")\n",
        "    print(\"\\n   2. Se HAI giÃ  i database ma sono in un'altra cartella:\")\n",
        "    print(\"      â†’ Verifica i path sopra\")\n",
        "    print(\"      â†’ Modifica le variabili DRIVE_TEXT_DB e DRIVE_IMAGE_DB\")\n",
        "    print(\"      â†’ Ri-esegui questo blocco\")\n",
        "    print(\"\\n   3. Se hai eseguito l'ingestione ma i database non appaiono:\")\n",
        "    print(\"      â†’ Ricarica Google Drive: drive.mount('/content/drive', force_remount=True)\")\n",
        "    print(\"      â†’ Ri-esegui questo blocco\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸŽ‰ CONFIGURAZIONE COMPLETA!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nâœ… Tutti i database sono presenti e pronti all'uso.\")\n",
        "    print(\"\\nðŸ“š Prossimi passi:\")\n",
        "    print(\"   1. Esegui BLOCCO 2 (Evaluation Framework)\")\n",
        "    print(\"   2. Esegui BLOCCO 2.5 (Advanced RAG Components)\")\n",
        "    print(\"   3. Poi scegli:\")\n",
        "    print(\"      â€¢ BLOCCO 3 (Test RAG) - per test singoli\")\n",
        "    print(\"      â€¢ BLOCCO 4 (Streamlit App) - per demo interattiva\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ’¡ TIPS:\")\n",
        "print(\"=\"*70)\n",
        "print(\"â€¢ Per cambiare threshold qualitÃ  immagini:\")\n",
        "print(\"  â†’ Modifica SOLO IMAGE_QUALITY_CONFIG (sopra)\")\n",
        "print(\"  â†’ Tutti i blocchi si aggiorneranno automaticamente!\")\n",
        "print(\"\\nâ€¢ Per modificare CLIP weight:\")\n",
        "print(f\"  â†’ Cambia CLIP_WEIGHT = {CLIP_WEIGHT} (sopra)\")\n",
        "print(\"\\nâ€¢ Per cambiare numero risultati:\")\n",
        "print(f\"  â†’ Stage 1: STAGE1_TOP_K = {STAGE1_TOP_K}\")\n",
        "print(f\"  â†’ Stage 2: STAGE2_TOP_K = {STAGE2_TOP_K}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nðŸŽ¯ VANTAGGI ARCHITETTURA CENTRALIZZATA:\")\n",
        "print(\"   âœ… Single Source of Truth (1 modifica invece di 5)\")\n",
        "print(\"   âœ… Zero rischio inconsistenze\")\n",
        "print(\"   âœ… Manutenzione semplificata\")\n",
        "print(\"   âœ… Production-ready best practice\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dro9weG2Pn0Z"
      },
      "source": [
        "#Cella di pulizia pre-ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcqyWGq9H7c0"
      },
      "outputs": [],
      "source": [
        "''' # ==========================================\n",
        "# ðŸ§¹ PULIZIA COMPLETA PRE-INGESTIONE\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ðŸ§¹ SCRIPT DI PULIZIA PRE-INGESTIONE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURAZIONE PATHS\n",
        "# ==========================================\n",
        "\n",
        "# Database locali (temp in /content)\n",
        "LOCAL_TEMP_PATHS = [\n",
        "    \"/content/temp_text_db\",\n",
        "    \"/content/temp_image_db\",\n",
        "]\n",
        "\n",
        "# Database Drive (permanenti)\n",
        "DRIVE_DB_PATHS = [\n",
        "    \"/content/drive/MyDrive/OCR/Testo_DB\",\n",
        "    \"/content/drive/MyDrive/OCR/Images_DB\",\n",
        "]\n",
        "\n",
        "# CLIP embeddings\n",
        "CLIP_JSON_PATH = \"/content/drive/MyDrive/OCR/clip_embeddings.json\"\n",
        "\n",
        "# OCR Output temporaneo\n",
        "OCR_OUTPUT_DIR = \"/content/drive/MyDrive/OCR_Output_Temp\"\n",
        "\n",
        "# Evaluation logs (OPZIONALE)\n",
        "EVAL_BASE = Path(\"/content/drive/MyDrive/OCR/evaluation\")\n",
        "EVAL_LOGS = EVAL_BASE / \"logs\"\n",
        "\n",
        "# ==========================================\n",
        "# FUNZIONI UTILITY\n",
        "# ==========================================\n",
        "\n",
        "def get_size_mb(path):\n",
        "    \"\"\"Calcola dimensione di file/directory in MB\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return 0\n",
        "\n",
        "    if os.path.isfile(path):\n",
        "        return os.path.getsize(path) / (1024 * 1024)\n",
        "\n",
        "    total = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.exists(fp):\n",
        "                total += os.path.getsize(fp)\n",
        "    return total / (1024 * 1024)\n",
        "\n",
        "def remove_path(path, name):\n",
        "    \"\"\"Rimuove path con gestione errori\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"   â­ï¸  {name}: giÃ  assente\")\n",
        "        return 0\n",
        "\n",
        "    size = get_size_mb(path)\n",
        "\n",
        "    try:\n",
        "        if os.path.isfile(path):\n",
        "            os.remove(path)\n",
        "        else:\n",
        "            shutil.rmtree(path)\n",
        "        print(f\"   âœ… {name}: rimosso ({size:.1f} MB)\")\n",
        "        return size\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ {name}: errore - {e}\")\n",
        "        return 0\n",
        "\n",
        "# ==========================================\n",
        "# SCANSIONE INIZIALE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ“Š SCANSIONE SPAZIO OCCUPATO...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "total_space = 0\n",
        "items_to_clean = []\n",
        "\n",
        "# 1. Database locali\n",
        "print(\"\\n1ï¸âƒ£  Database Locali (/content):\")\n",
        "for path in LOCAL_TEMP_PATHS:\n",
        "    size = get_size_mb(path)\n",
        "    total_space += size\n",
        "    status = \"âœ… Presente\" if os.path.exists(path) else \"â­ï¸  Assente\"\n",
        "    print(f\"   {status}: {os.path.basename(path)} ({size:.1f} MB)\")\n",
        "    if os.path.exists(path):\n",
        "        items_to_clean.append(('local', path, os.path.basename(path)))\n",
        "\n",
        "# 2. Database Drive\n",
        "print(\"\\n2ï¸âƒ£  Database Drive (permanenti):\")\n",
        "for path in DRIVE_DB_PATHS:\n",
        "    size = get_size_mb(path)\n",
        "    total_space += size\n",
        "    status = \"âœ… Presente\" if os.path.exists(path) else \"â­ï¸  Assente\"\n",
        "    print(f\"   {status}: {os.path.basename(path)} ({size:.1f} MB)\")\n",
        "    if os.path.exists(path):\n",
        "        items_to_clean.append(('drive_db', path, os.path.basename(path)))\n",
        "\n",
        "# 3. CLIP embeddings\n",
        "print(\"\\n3ï¸âƒ£  CLIP Embeddings:\")\n",
        "size = get_size_mb(CLIP_JSON_PATH)\n",
        "total_space += size\n",
        "status = \"âœ… Presente\" if os.path.exists(CLIP_JSON_PATH) else \"â­ï¸  Assente\"\n",
        "print(f\"   {status}: clip_embeddings.json ({size:.1f} MB)\")\n",
        "if os.path.exists(CLIP_JSON_PATH):\n",
        "    items_to_clean.append(('clip', CLIP_JSON_PATH, 'clip_embeddings.json'))\n",
        "\n",
        "# 4. OCR Output temporaneo\n",
        "print(\"\\n4ï¸âƒ£  OCR Output Temporaneo:\")\n",
        "size = get_size_mb(OCR_OUTPUT_DIR)\n",
        "total_space += size\n",
        "status = \"âœ… Presente\" if os.path.exists(OCR_OUTPUT_DIR) else \"â­ï¸  Assente\"\n",
        "print(f\"   {status}: OCR_Output_Temp ({size:.1f} MB)\")\n",
        "if os.path.exists(OCR_OUTPUT_DIR):\n",
        "    items_to_clean.append(('ocr_temp', OCR_OUTPUT_DIR, 'OCR_Output_Temp'))\n",
        "\n",
        "# 5. Evaluation logs\n",
        "print(\"\\n5ï¸âƒ£  Evaluation Logs:\")\n",
        "size = get_size_mb(EVAL_LOGS)\n",
        "total_space += size\n",
        "status = \"âœ… Presente\" if os.path.exists(EVAL_LOGS) else \"â­ï¸  Assente\"\n",
        "print(f\"   {status}: evaluation/logs ({size:.1f} MB)\")\n",
        "if os.path.exists(EVAL_LOGS):\n",
        "    items_to_clean.append(('eval', EVAL_LOGS, 'evaluation/logs'))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"ðŸ“¦ TOTALE SPAZIO OCCUPATO: {total_space:.1f} MB\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# CONFERMA UTENTE\n",
        "# ==========================================\n",
        "\n",
        "if not items_to_clean:\n",
        "    print(\"\\nâœ… Nessun file da pulire. Tutto giÃ  pulito!\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  ATTENZIONE: Questa operazione rimuoverÃ  {len(items_to_clean)} elementi.\")\n",
        "    print(f\"   Spazio liberato: ~{total_space:.1f} MB\")\n",
        "    print(\"\\nðŸ“‹ Elementi da rimuovere:\")\n",
        "\n",
        "    # Raggruppa per tipo\n",
        "    local_items = [x for x in items_to_clean if x[0] == 'local']\n",
        "    drive_db_items = [x for x in items_to_clean if x[0] == 'drive_db']\n",
        "    clip_items = [x for x in items_to_clean if x[0] == 'clip']\n",
        "    ocr_items = [x for x in items_to_clean if x[0] == 'ocr_temp']\n",
        "    eval_items = [x for x in items_to_clean if x[0] == 'eval']\n",
        "\n",
        "    if local_items:\n",
        "        print(f\"\\n   ðŸ—‚ï¸  Database Locali ({len(local_items)}):\")\n",
        "        for _, path, name in local_items:\n",
        "            print(f\"      â€¢ {name}\")\n",
        "\n",
        "    if drive_db_items:\n",
        "        print(f\"\\n   ðŸ’¾ Database Drive ({len(drive_db_items)}):\")\n",
        "        for _, path, name in drive_db_items:\n",
        "            print(f\"      â€¢ {name}\")\n",
        "\n",
        "    if clip_items:\n",
        "        print(f\"\\n   ðŸ–¼ï¸  CLIP Embeddings ({len(clip_items)}):\")\n",
        "        for _, path, name in clip_items:\n",
        "            print(f\"      â€¢ {name}\")\n",
        "\n",
        "    if ocr_items:\n",
        "        print(f\"\\n   ðŸ“„ OCR Output ({len(ocr_items)}):\")\n",
        "        for _, path, name in ocr_items:\n",
        "            print(f\"      â€¢ {name}\")\n",
        "\n",
        "    if eval_items:\n",
        "        print(f\"\\n   ðŸ“Š Evaluation Logs ({len(eval_items)}):\")\n",
        "        for _, path, name in eval_items:\n",
        "            print(f\"      â€¢ {name}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸ”´ QUESTA OPERAZIONE Ãˆ IRREVERSIBILE!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # ==========================================\n",
        "    # OPZIONI DI PULIZIA\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\nðŸ“ Opzioni disponibili:\")\n",
        "    print(\"   1ï¸âƒ£  PULIZIA COMPLETA (rimuove tutto)\")\n",
        "    print(\"   2ï¸âƒ£  PULIZIA SELETTIVA (scegli cosa rimuovere)\")\n",
        "    print(\"   3ï¸âƒ£  SOLO DATABASE (mantieni OCR output e logs)\")\n",
        "    print(\"   4ï¸âƒ£  ANNULLA (non fare nulla)\")\n",
        "\n",
        "    choice = input(\"\\nâ“ Scegli un'opzione (1-4): \").strip()\n",
        "\n",
        "    # ==========================================\n",
        "    # ESECUZIONE PULIZIA\n",
        "    # ==========================================\n",
        "\n",
        "    freed_space = 0\n",
        "\n",
        "    if choice == '1':\n",
        "        # PULIZIA COMPLETA\n",
        "        print(\"\\nðŸš€ PULIZIA COMPLETA IN CORSO...\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        confirm = input(\"\\nâš ï¸  Sei SICURO? Digita 'DELETE' per confermare: \").strip()\n",
        "\n",
        "        if confirm == 'DELETE':\n",
        "            for item_type, path, name in items_to_clean:\n",
        "                freed_space += remove_path(path, name)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(f\"âœ… PULIZIA COMPLETATA!\")\n",
        "            print(f\"ðŸ’¾ Spazio liberato: {freed_space:.1f} MB\")\n",
        "            print(\"=\" * 70)\n",
        "        else:\n",
        "            print(\"\\nâŒ Operazione annullata (conferma non corretta)\")\n",
        "\n",
        "    elif choice == '2':\n",
        "        # PULIZIA SELETTIVA\n",
        "        print(\"\\nðŸŽ¯ PULIZIA SELETTIVA\")\n",
        "        print(\"-\" * 70)\n",
        "        print(\"\\nPer ogni elemento, scegli:\")\n",
        "        print(\"   â€¢ 's' = SÃ¬, rimuovi\")\n",
        "        print(\"   â€¢ 'n' = No, mantieni\")\n",
        "        print()\n",
        "\n",
        "        for item_type, path, name in items_to_clean:\n",
        "            response = input(f\"â“ Rimuovere '{name}'? (s/n): \").strip().lower()\n",
        "            if response == 's':\n",
        "                freed_space += remove_path(path, name)\n",
        "            else:\n",
        "                print(f\"   â­ï¸  {name}: mantenuto\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"âœ… PULIZIA SELETTIVA COMPLETATA!\")\n",
        "        print(f\"ðŸ’¾ Spazio liberato: {freed_space:.1f} MB\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    elif choice == '3':\n",
        "        # SOLO DATABASE\n",
        "        print(\"\\nðŸ—„ï¸  PULIZIA SOLO DATABASE\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        confirm = input(\"\\nâš ï¸  Rimuovere tutti i database? Digita 'YES' per confermare: \").strip()\n",
        "\n",
        "        if confirm == 'YES':\n",
        "            # Rimuovi solo database e CLIP\n",
        "            for item_type, path, name in items_to_clean:\n",
        "                if item_type in ['local', 'drive_db', 'clip']:\n",
        "                    freed_space += remove_path(path, name)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(f\"âœ… DATABASE PULITI!\")\n",
        "            print(f\"ðŸ’¾ Spazio liberato: {freed_space:.1f} MB\")\n",
        "            print(f\"ðŸ“„ OCR Output e Logs: mantenuti\")\n",
        "            print(\"=\" * 70)\n",
        "        else:\n",
        "            print(\"\\nâŒ Operazione annullata (conferma non corretta)\")\n",
        "\n",
        "    elif choice == '4':\n",
        "        print(\"\\nâœ… Operazione annullata dall'utente\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nâŒ Opzione non valida. Operazione annullata.\")\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA FINALE\n",
        "# ==========================================\n",
        "\n",
        "if 'freed_space' in locals() and freed_space > 0:\n",
        "    print(\"\\nðŸ” VERIFICA FINALE...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    remaining_items = []\n",
        "    for item_type, path, name in items_to_clean:\n",
        "        if os.path.exists(path):\n",
        "            remaining_items.append(name)\n",
        "\n",
        "    if remaining_items:\n",
        "        print(f\"\\nâš ï¸  {len(remaining_items)} elementi ancora presenti:\")\n",
        "        for name in remaining_items:\n",
        "            print(f\"   â€¢ {name}\")\n",
        "    else:\n",
        "        print(\"\\nâœ… Tutti gli elementi selezionati sono stati rimossi!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸ“Š STATO FINALE:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"   â€¢ Spazio liberato: {freed_space:.1f} MB\")\n",
        "    print(f\"   â€¢ Database pronti per nuova ingestione: âœ…\")\n",
        "    print(\"\\nðŸ’¡ Prossimi passi:\")\n",
        "    print(\"   1. Esegui BLOCCO 0 (Setup)\")\n",
        "    print(\"   2. Esegui BLOCCO 1 (Config) con nuovi parametri\")\n",
        "    print(\"   3. Esegui BLOCCO INGESTIONE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Script completato!\") '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f4VKObvPuGM"
      },
      "source": [
        "#cella di ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1n8IxQ00l4z"
      },
      "outputs": [],
      "source": [
        "''' # ==========================================\n",
        "# BLOCCO INGESTIONE: DOLPHIN OCR + DATABASE CREATION\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import gc\n",
        "import torch\n",
        "import subprocess\n",
        "import re\n",
        "import json\n",
        "from urllib.parse import unquote\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "import fitz  # PyMuPDF per batch splitting\n",
        "\n",
        "# Sopprimi warning\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ðŸš€ BLOCCO INGESTIONE: Dolphin OCR + Database Creation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA BLOCCO 1 (Config) ESEGUITO\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nâš™ï¸ Verifica configurazione...\")\n",
        "\n",
        "try:\n",
        "    # Verifica che le variabili dal BLOCCO 1 esistano\n",
        "    _required_vars = [\n",
        "        ('PDF_INPUT_DIR', PDF_INPUT_DIR),\n",
        "        ('OCR_OUTPUT_DIR', OCR_OUTPUT_DIR),\n",
        "        ('LOCAL_TEXT_DB', LOCAL_TEXT_DB),\n",
        "        ('LOCAL_IMAGE_DB', LOCAL_IMAGE_DB),\n",
        "        ('DRIVE_TEXT_DB', DRIVE_TEXT_DB),\n",
        "        ('DRIVE_IMAGE_DB', DRIVE_IMAGE_DB),\n",
        "        ('DRIVE_CLIP_JSON', DRIVE_CLIP_JSON),\n",
        "        ('TEXT_EMBED_MODEL', TEXT_EMBED_MODEL),\n",
        "        ('CLIP_MODEL_NAME', CLIP_MODEL_NAME),\n",
        "        ('TEXT_COLLECTION_NAME', TEXT_COLLECTION_NAME),\n",
        "        ('IMAGE_COLLECTION_NAME', IMAGE_COLLECTION_NAME),\n",
        "        ('PAGES_PER_BATCH', PAGES_PER_BATCH),\n",
        "        ('DOLPHIN_TIMEOUT', DOLPHIN_TIMEOUT),\n",
        "        ('CHUNK_SIZE', CHUNK_SIZE),\n",
        "        ('CHUNK_OVERLAP', CHUNK_OVERLAP),\n",
        "        ('CLIP_SIZE_KB_MIN', CLIP_SIZE_KB_MIN),\n",
        "        ('CLIP_WIDTH_MIN', CLIP_WIDTH_MIN),\n",
        "        ('CLIP_HEIGHT_MIN', CLIP_HEIGHT_MIN),\n",
        "        ('CLIP_CAPTION_MAX_LEN', CLIP_CAPTION_MAX_LEN),\n",
        "    ]\n",
        "\n",
        "    print(\"âœ… Configurazione da BLOCCO 1 trovata\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âŒ ERRORE: BLOCCO 1 (Config) non Ã¨ stato eseguito!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nVariabile mancante: {e}\")\n",
        "    print(\"\\nðŸ”§ SOLUZIONE:\")\n",
        "    print(\"   1. Torna indietro\")\n",
        "    print(\"   2. Esegui il BLOCCO 1 (Config)\")\n",
        "    print(\"   3. Poi riesegui questo blocco\\n\")\n",
        "    print(\"=\"*70)\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA ESISTENZA DATABASE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ“Š Verifica database esistenti...\")\n",
        "\n",
        "text_db_exists = os.path.exists(DRIVE_TEXT_DB)\n",
        "img_db_exists = os.path.exists(DRIVE_IMAGE_DB)\n",
        "clip_json_exists = os.path.exists(DRIVE_CLIP_JSON)\n",
        "\n",
        "print(f\"   {'âœ…' if text_db_exists else 'âŒ'} Text DB: {DRIVE_TEXT_DB}\")\n",
        "print(f\"   {'âœ…' if img_db_exists else 'âŒ'} Image DB: {DRIVE_IMAGE_DB}\")\n",
        "print(f\"   {'âœ…' if clip_json_exists else 'âŒ'} CLIP JSON: {DRIVE_CLIP_JSON}\")\n",
        "\n",
        "if text_db_exists and img_db_exists:\n",
        "    print(\"\\nâš ï¸ ATTENZIONE: Database giÃ  esistenti!\")\n",
        "    print(\"   Se continui, i database esistenti verranno SOVRASCRITTI.\")\n",
        "    print(\"   Questo processo richiede 30-60 minuti.\")\n",
        "\n",
        "    response = input(\"\\nâ“ Vuoi continuare e sovrascrivere? (yes/no): \")\n",
        "\n",
        "    if response.lower() not in ['yes', 'y']:\n",
        "        print(\"\\nâœ… Operazione annullata. Database esistenti preservati.\")\n",
        "        print(\"   Puoi procedere con i BLOCCHI 2, 3 o 4.\")\n",
        "        raise SystemExit(\"User cancelled operation\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ Rimozione database esistenti...\")\n",
        "        if os.path.exists(DRIVE_TEXT_DB):\n",
        "            shutil.rmtree(DRIVE_TEXT_DB)\n",
        "            print(f\"   ðŸ—‘ï¸ Rimosso: {DRIVE_TEXT_DB}\")\n",
        "        if os.path.exists(DRIVE_IMAGE_DB):\n",
        "            shutil.rmtree(DRIVE_IMAGE_DB)\n",
        "            print(f\"   ðŸ—‘ï¸ Rimosso: {DRIVE_IMAGE_DB}\")\n",
        "        if os.path.exists(DRIVE_CLIP_JSON):\n",
        "            os.remove(DRIVE_CLIP_JSON)\n",
        "            print(f\"   ðŸ—‘ï¸ Rimosso: {DRIVE_CLIP_JSON}\")\n",
        "\n",
        "# ==========================================\n",
        "# PRINT CONFIGURAZIONE\n",
        "# ==========================================\n",
        "\n",
        "print(f\"\"\"\n",
        "{\"=\"*70}\n",
        "âš™ï¸ CONFIGURAZIONE INGESTIONE:\n",
        "{\"=\"*70}\n",
        "\n",
        "ðŸ“‚ Input/Output:\n",
        "   â€¢ PDF Input: {PDF_INPUT_DIR}\n",
        "   â€¢ OCR Output: {OCR_OUTPUT_DIR}\n",
        "   â€¢ Drive Text DB: {DRIVE_TEXT_DB}\n",
        "   â€¢ Drive Image DB: {DRIVE_IMAGE_DB}\n",
        "   â€¢ CLIP JSON: {DRIVE_CLIP_JSON}\n",
        "\n",
        "ðŸ¤– Modelli:\n",
        "   â€¢ Text Embed: {TEXT_EMBED_MODEL}\n",
        "   â€¢ CLIP: {CLIP_MODEL_NAME}\n",
        "\n",
        "ðŸ¬ Dolphin Batch:\n",
        "   â€¢ Pagine/batch: {PAGES_PER_BATCH}\n",
        "   â€¢ Timeout: {DOLPHIN_TIMEOUT}s\n",
        "\n",
        "ðŸ“ Text Chunking:\n",
        "   â€¢ Chunk size: {CHUNK_SIZE}\n",
        "   â€¢ Overlap: {CHUNK_OVERLAP}\n",
        "\n",
        "ðŸ–¼ï¸ CLIP Smart Threshold:\n",
        "   â€¢ Min size: {CLIP_SIZE_KB_MIN} KB\n",
        "   â€¢ Min width: {CLIP_WIDTH_MIN} px\n",
        "   â€¢ Min height: {CLIP_HEIGHT_MIN} px\n",
        "   â€¢ Caption max: {CLIP_CAPTION_MAX_LEN} char\n",
        "\n",
        "{\"=\"*70}\n",
        "\"\"\")\n",
        "\n",
        "# ==========================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def split_pdf_in_batches(pdf_path, output_dir, pages_per_batch):\n",
        "    \"\"\"\n",
        "    Divide PDF in batch piÃ¹ piccoli per Dolphin\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    total_pages = len(doc)\n",
        "\n",
        "    batch_paths = []\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for start in range(0, total_pages, pages_per_batch):\n",
        "        end = min(start + pages_per_batch, total_pages)\n",
        "        batch_name = f\"batch_{start:04d}_{end:04d}.pdf\"\n",
        "        batch_path = os.path.join(output_dir, batch_name)\n",
        "\n",
        "        # Crea PDF batch\n",
        "        batch_doc = fitz.open()\n",
        "        batch_doc.insert_pdf(doc, from_page=start, to_page=end-1)\n",
        "        batch_doc.save(batch_path)\n",
        "        batch_doc.close()\n",
        "\n",
        "        batch_paths.append((batch_path, start, end))\n",
        "\n",
        "    doc.close()\n",
        "    return batch_paths, total_pages\n",
        "\n",
        "def run_dolphin_on_batch(batch_pdf, output_dir, timeout):\n",
        "    \"\"\"\n",
        "    Esegue Dolphin su un singolo batch\n",
        "    \"\"\"\n",
        "    cmd = f\"\"\"\n",
        "    export TF_CPP_MIN_LOG_LEVEL=3\n",
        "    export CUDA_VISIBLE_DEVICES=0\n",
        "    timeout {timeout} python /content/Dolphin/demo_page.py \\\n",
        "        --model_path /content/hf_model \\\n",
        "        --input_path \"{batch_pdf}\" \\\n",
        "        --save_dir \"{output_dir}\" 2>&1\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=timeout + 10\n",
        "        )\n",
        "\n",
        "        # Verifica successo\n",
        "        md_files = glob.glob(os.path.join(output_dir, \"markdown\", \"*.md\"))\n",
        "        if md_files and result.returncode != 124:\n",
        "            return True, len(md_files)\n",
        "        return False, 0\n",
        "    except Exception as e:\n",
        "        return False, 0\n",
        "\n",
        "def merge_batch_results(manual_name, batch_results, final_output_dir):\n",
        "    \"\"\"\n",
        "    Unisce i risultati dei batch in un unico output\n",
        "    \"\"\"\n",
        "    os.makedirs(final_output_dir, exist_ok=True)\n",
        "    final_md_dir = os.path.join(final_output_dir, \"markdown\")\n",
        "    os.makedirs(final_md_dir, exist_ok=True)\n",
        "\n",
        "    all_md_content = []\n",
        "    page_counter = 1\n",
        "\n",
        "    # Merge markdown files\n",
        "    for batch_dir, start_page, end_page in sorted(batch_results):\n",
        "        batch_md_dir = os.path.join(batch_dir, \"markdown\")\n",
        "        if os.path.exists(batch_md_dir):\n",
        "            md_files = sorted(glob.glob(os.path.join(batch_md_dir, \"*.md\")))\n",
        "\n",
        "            for md_file in md_files:\n",
        "                with open(md_file, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                    all_md_content.append(f\"## Pagina {page_counter}\\n\\n{content}\\n\")\n",
        "                    page_counter += 1\n",
        "\n",
        "        # Copia immagini\n",
        "        batch_images = glob.glob(os.path.join(batch_dir, \"*.png\"))\n",
        "        for img in batch_images:\n",
        "            shutil.copy(img, final_output_dir)\n",
        "\n",
        "    # Salva markdown unificato\n",
        "    unified_md = os.path.join(final_md_dir, f\"{manual_name}.md\")\n",
        "    with open(unified_md, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(all_md_content))\n",
        "\n",
        "    # Pulizia batch temporanei\n",
        "    for batch_dir, _, _ in batch_results:\n",
        "        if os.path.exists(batch_dir):\n",
        "            shutil.rmtree(batch_dir)\n",
        "\n",
        "def save_to_drive(local_path, drive_path):\n",
        "    \"\"\"Salva su Drive\"\"\"\n",
        "    print(f\"ðŸ’¾ Salvataggio: {os.path.basename(drive_path)}...\")\n",
        "    if os.path.exists(drive_path):\n",
        "        shutil.rmtree(drive_path)\n",
        "    shutil.copytree(local_path, drive_path)\n",
        "    print(\"âœ… Salvato.\")\n",
        "\n",
        "def should_use_clip(img_path, caption):\n",
        "    \"\"\"\n",
        "    Smart threshold per decidere se usare CLIP\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check dimensione file\n",
        "        size_kb = os.path.getsize(img_path) / 1024\n",
        "        if size_kb < CLIP_SIZE_KB_MIN:\n",
        "            return False\n",
        "\n",
        "        # Check dimensioni immagine\n",
        "        with Image.open(img_path) as img:\n",
        "            width, height = img.size\n",
        "            if width < CLIP_WIDTH_MIN or height < CLIP_HEIGHT_MIN:\n",
        "                return False\n",
        "\n",
        "        # Check caption\n",
        "        caption_short = len(caption.strip()) < CLIP_CAPTION_MAX_LEN\n",
        "        caption_generic = caption.lower().startswith((\"figura\", \"fig.\", \"immagine\", \"image\"))\n",
        "\n",
        "        # Usa CLIP se: grande + (caption corta O generica)\n",
        "        return caption_short or caption_generic\n",
        "\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# ==========================================\n",
        "# FASE 1: DOLPHIN OCR BATCH\n",
        "# ==========================================\n",
        "\n",
        "def run_batch_dolphin_ocr():\n",
        "    \"\"\"\n",
        "    Processa PDF con Dolphin in batch\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ¬ FASE 1: DOLPHIN OCR BATCH PROCESSING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if not os.path.exists(PDF_INPUT_DIR):\n",
        "        print(f\"âŒ Cartella PDF non trovata: {PDF_INPUT_DIR}\")\n",
        "        return False\n",
        "\n",
        "    pdf_files = glob.glob(f\"{PDF_INPUT_DIR}/*.pdf\")\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ Nessun PDF trovato\")\n",
        "        return False\n",
        "\n",
        "    print(f\"\\nðŸ“„ Trovati {len(pdf_files)} PDF da processare\")\n",
        "    os.makedirs(OCR_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    pbar_pdf = tqdm(pdf_files, desc=\"ðŸ”„ OCR Batch Dolphin\", unit=\"PDF\")\n",
        "\n",
        "    for pdf in pbar_pdf:\n",
        "        name = Path(pdf).stem\n",
        "        final_output = os.path.join(OCR_OUTPUT_DIR, name)\n",
        "\n",
        "        pbar_pdf.set_description(f\"ðŸ”„ {name[:20]}...\")\n",
        "\n",
        "        # Skip se giÃ  processato\n",
        "        if os.path.exists(final_output):\n",
        "            md_files = glob.glob(os.path.join(final_output, \"markdown\", \"*.md\"))\n",
        "            if md_files:\n",
        "                pbar_pdf.write(f\"â­ï¸ Skip: {name} (giÃ  processato)\")\n",
        "                continue\n",
        "\n",
        "        pdf_size_mb = os.path.getsize(pdf) / (1024 * 1024)\n",
        "        pbar_pdf.write(f\"\\nðŸ“‹ {name} ({pdf_size_mb:.1f} MB)\")\n",
        "\n",
        "        # Split in batches\n",
        "        temp_batch_dir = os.path.join(OCR_OUTPUT_DIR, f\"{name}_batches\")\n",
        "        batch_paths, total_pages = split_pdf_in_batches(pdf, temp_batch_dir, PAGES_PER_BATCH)\n",
        "\n",
        "        num_batches = len(batch_paths)\n",
        "        pbar_pdf.write(f\"   ðŸ“¦ Diviso in {num_batches} batch ({PAGES_PER_BATCH} pagine/batch)\")\n",
        "\n",
        "        # Processa ogni batch\n",
        "        batch_results = []\n",
        "        pbar_batch = tqdm(batch_paths, desc=\"   ðŸ¬ Batch\", leave=False, unit=\"batch\")\n",
        "\n",
        "        for batch_pdf, start, end in pbar_batch:\n",
        "            batch_name = f\"pg{start}-{end}\"\n",
        "            batch_output = os.path.join(temp_batch_dir, f\"output_{batch_name}\")\n",
        "\n",
        "            pbar_batch.set_description(f\"   ðŸ¬ Pagine {start}-{end}\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            success, num_pages = run_dolphin_on_batch(batch_pdf, batch_output, DOLPHIN_TIMEOUT)\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            if success:\n",
        "                pbar_pdf.write(f\"      âœ… Batch {batch_name}: {num_pages} pagine ({elapsed/60:.1f} min)\")\n",
        "                batch_results.append((batch_output, start, end))\n",
        "            else:\n",
        "                pbar_pdf.write(f\"      âš ï¸ Batch {batch_name}: fallito/timeout\")\n",
        "\n",
        "        pbar_batch.close()\n",
        "\n",
        "        # Merge risultati\n",
        "        if batch_results:\n",
        "            pbar_pdf.write(f\"   ðŸ”— Merge {len(batch_results)} batch...\")\n",
        "            merge_batch_results(name, batch_results, final_output)\n",
        "            pbar_pdf.write(f\"   âœ… {name} completato\")\n",
        "        else:\n",
        "            pbar_pdf.write(f\"   âŒ {name} fallito completamente\")\n",
        "\n",
        "        # Cleanup batch directory\n",
        "        if os.path.exists(temp_batch_dir):\n",
        "            shutil.rmtree(temp_batch_dir)\n",
        "\n",
        "    pbar_pdf.close()\n",
        "\n",
        "    print(\"\\nâœ… FASE 1 COMPLETATA: Dolphin OCR\")\n",
        "    return True\n",
        "\n",
        "# ==========================================\n",
        "# FASE 2: TEXT DATABASE\n",
        "# ==========================================\n",
        "\n",
        "def create_text_db():\n",
        "    \"\"\"Crea database testuale\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“š FASE 2: CREAZIONE TEXT DATABASE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    from langchain_community.document_loaders import TextLoader\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    try:\n",
        "        from langchain_chroma import Chroma\n",
        "    except ImportError:\n",
        "        from langchain_community.vectorstores import Chroma\n",
        "    from langchain_core.documents import Document\n",
        "\n",
        "    md_files = glob.glob(f\"{OCR_OUTPUT_DIR}/**/*.md\", recursive=True)\n",
        "\n",
        "    if not md_files:\n",
        "        print(\"âŒ Nessun file markdown trovato!\")\n",
        "        return False\n",
        "\n",
        "    docs = []\n",
        "    for md in tqdm(md_files, desc=\"ðŸ“„ Caricamento MD\", unit=\"file\"):\n",
        "        try:\n",
        "            loader = TextLoader(md, encoding='utf-8')\n",
        "            loaded_docs = loader.load()\n",
        "            manual_name = Path(md).parents[1].name\n",
        "\n",
        "            for doc in loaded_docs:\n",
        "                doc.metadata['manual'] = manual_name\n",
        "                doc.metadata['source'] = md\n",
        "                doc.metadata['language'] = 'it'\n",
        "\n",
        "            docs.extend(loaded_docs)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if not docs:\n",
        "        print(\"âŒ Nessun documento caricato\")\n",
        "        return False\n",
        "\n",
        "    print(f\"   ðŸ“ Chunking {len(docs)} documenti...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    print(f\"   âœ… {len(chunks)} chunks creati\")\n",
        "\n",
        "    print(\"   ðŸ”§ Caricamento Embedding...\")\n",
        "    embed_model = HuggingFaceEmbeddings(\n",
        "        model_name=TEXT_EMBED_MODEL,\n",
        "        model_kwargs={'device': 'cuda'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    if os.path.exists(LOCAL_TEXT_DB):\n",
        "        shutil.rmtree(LOCAL_TEXT_DB)\n",
        "        time.sleep(2)\n",
        "    os.makedirs(LOCAL_TEXT_DB, exist_ok=True)\n",
        "\n",
        "    print(\"   âš¡ Creazione vector store...\")\n",
        "    Chroma.from_documents(\n",
        "        chunks,\n",
        "        embed_model,\n",
        "        persist_directory=LOCAL_TEXT_DB,\n",
        "        collection_name=TEXT_COLLECTION_NAME,\n",
        "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"âœ… Text DB creato ({len(chunks)} chunks)\")\n",
        "\n",
        "    save_to_drive(LOCAL_TEXT_DB, DRIVE_TEXT_DB)\n",
        "\n",
        "    del embed_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\nâœ… FASE 2 COMPLETATA: Text Database\")\n",
        "    return True\n",
        "\n",
        "# ==========================================\n",
        "# FASE 3: IMAGE DATABASE + CLIP\n",
        "# ==========================================\n",
        "\n",
        "def create_image_db_with_selective_clip():\n",
        "    \"\"\"\n",
        "    Crea DB immagini con embedding testuale + CLIP selettivo\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ–¼ï¸ FASE 3: CREAZIONE IMAGE DATABASE + CLIP SELETTIVO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    from langchain_community.document_loaders import TextLoader\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    try:\n",
        "        from langchain_chroma import Chroma\n",
        "    except ImportError:\n",
        "        from langchain_community.vectorstores import Chroma\n",
        "    from langchain_core.documents import Document\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "    # Trova tutte le immagini\n",
        "    all_pngs = glob.glob(f\"{OCR_OUTPUT_DIR}/**/*.png\", recursive=True)\n",
        "    all_jpgs = glob.glob(f\"{OCR_OUTPUT_DIR}/**/*.jpg\", recursive=True)\n",
        "    all_images = all_pngs + all_jpgs\n",
        "\n",
        "    if not all_images:\n",
        "        print(\"âŒ Nessuna immagine trovata\")\n",
        "        return False\n",
        "\n",
        "    img_map = {os.path.basename(p): p for p in all_images}\n",
        "    print(f\"   âœ… Mappate {len(img_map)} immagini\")\n",
        "\n",
        "    # Estrai metadata immagini dai markdown\n",
        "    all_images_data = []\n",
        "    md_files = glob.glob(f\"{OCR_OUTPUT_DIR}/**/*.md\", recursive=True)\n",
        "\n",
        "    for md_file in md_files:\n",
        "        manual_name = Path(md_file).parents[1].name\n",
        "        with open(md_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        images_found = re.findall(r'!\\s*\\[(.*?)\\]\\s*\\((.*?\\.(?:png|jpg|jpeg))\\)', content, re.IGNORECASE)\n",
        "\n",
        "        count = 0\n",
        "        for caption, rel_path in images_found:\n",
        "            clean_filename = unquote(os.path.basename(rel_path))\n",
        "\n",
        "            if clean_filename in img_map:\n",
        "                full_img_path = img_map[clean_filename]\n",
        "                idx = content.find(rel_path)\n",
        "                start = max(0, idx - 200)\n",
        "                end = min(len(content), idx + 200)\n",
        "                context = content[start:end].replace(\"\\n\", \" \").strip()\n",
        "\n",
        "                all_images_data.append({\n",
        "                    \"path\": full_img_path,\n",
        "                    \"manual\": manual_name,\n",
        "                    \"caption\": caption,\n",
        "                    \"context\": context\n",
        "                })\n",
        "                count += 1\n",
        "\n",
        "        print(f\"   ðŸ“„ {manual_name}: {count} immagini\")\n",
        "\n",
        "    if not all_images_data:\n",
        "        print(\"âŒ Nessuna immagine da indicizzare\")\n",
        "        return False\n",
        "\n",
        "    # Fase 1: Crea DB con embedding testuale per TUTTE le immagini\n",
        "    print(f\"\\n   ðŸ”§ Caricamento Text Embedding...\")\n",
        "    text_embed = HuggingFaceEmbeddings(\n",
        "        model_name=TEXT_EMBED_MODEL,\n",
        "        model_kwargs={'device': 'cuda'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    clip_candidates = []  # Immagini che avranno CLIP\n",
        "\n",
        "    for i, img_data in enumerate(tqdm(all_images_data, desc=\"   ðŸ“Š Analisi immagini\", unit=\"img\")):\n",
        "        text_content = f\"{img_data['caption']}. {img_data['context']}\"\n",
        "\n",
        "        # Determina se usare CLIP\n",
        "        use_clip = should_use_clip(img_data['path'], img_data['caption'])\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=text_content,\n",
        "            metadata={\n",
        "                \"image_path\": img_data['path'],\n",
        "                \"manual\": img_data['manual'],\n",
        "                \"caption\": img_data['caption'],\n",
        "                \"type\": \"image\",\n",
        "                \"has_clip\": use_clip,  # Flag per indicare CLIP disponibile\n",
        "                \"image_id\": f\"img_{i}\"\n",
        "            }\n",
        "        )\n",
        "        docs.append(doc)\n",
        "\n",
        "        if use_clip:\n",
        "            clip_candidates.append((f\"img_{i}\", img_data['path']))\n",
        "\n",
        "    print(f\"   âœ… {len(docs)} immagini totali\")\n",
        "    print(f\"   ðŸŽ¯ {len(clip_candidates)} candidate per CLIP ({len(clip_candidates)/len(docs)*100:.1f}%)\")\n",
        "\n",
        "    # Crea DB con embedding testuale\n",
        "    if os.path.exists(LOCAL_IMAGE_DB):\n",
        "        shutil.rmtree(LOCAL_IMAGE_DB)\n",
        "        time.sleep(2)\n",
        "    os.makedirs(LOCAL_IMAGE_DB, exist_ok=True)\n",
        "\n",
        "    print(\"   âš¡ Creazione vector store (testuale)...\")\n",
        "    Chroma.from_documents(\n",
        "        docs,\n",
        "        text_embed,\n",
        "        persist_directory=LOCAL_IMAGE_DB,\n",
        "        collection_name=IMAGE_COLLECTION_NAME,\n",
        "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"âœ… Image DB creato (testuale)\")\n",
        "\n",
        "    del text_embed\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Fase 2: Pre-calcola CLIP embeddings per candidati selezionati\n",
        "    if clip_candidates:\n",
        "        print(f\"\\n   ðŸ”§ Caricamento CLIP per {len(clip_candidates)} immagini selezionate...\")\n",
        "\n",
        "        clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
        "        clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "        clip_model = clip_model.to('cuda')\n",
        "        clip_model.eval()\n",
        "\n",
        "        clip_embeddings = {}\n",
        "        batch_size = 8\n",
        "\n",
        "        pbar = tqdm(range(0, len(clip_candidates), batch_size),\n",
        "                    desc=\"   âš¡ CLIP Embedding\", unit=\"batch\")\n",
        "\n",
        "        for i in pbar:\n",
        "            batch = clip_candidates[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            batch_ids = []\n",
        "\n",
        "            for img_id, img_path in batch:\n",
        "                try:\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    if max(img.size) > 1024:\n",
        "                        img.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
        "                    batch_images.append(img)\n",
        "                    batch_ids.append(img_id)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if batch_images:\n",
        "                inputs = clip_processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    image_features = clip_model.get_image_features(**inputs)\n",
        "                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                for j, img_id in enumerate(batch_ids):\n",
        "                    clip_embeddings[img_id] = image_features[j].cpu().numpy().tolist()\n",
        "\n",
        "            pbar.set_postfix({\"embeddings\": len(clip_embeddings)})\n",
        "\n",
        "        pbar.close()\n",
        "        print(f\"   âœ… {len(clip_embeddings)} CLIP embeddings generati\")\n",
        "\n",
        "        # Salva CLIP embeddings in JSON separato\n",
        "        clip_json_path = DRIVE_CLIP_JSON\n",
        "        os.makedirs(os.path.dirname(clip_json_path), exist_ok=True)\n",
        "        with open(clip_json_path, 'w') as f:\n",
        "            json.dump(clip_embeddings, f)\n",
        "        print(f\"   ðŸ’¾ CLIP embeddings salvati: {clip_json_path}\")\n",
        "\n",
        "        del clip_model, clip_processor\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    save_to_drive(LOCAL_IMAGE_DB, DRIVE_IMAGE_DB)\n",
        "\n",
        "    print(\"\\nâœ… FASE 3 COMPLETATA: Image Database + CLIP\")\n",
        "    return True\n",
        "\n",
        "# ==========================================\n",
        "# ESECUZIONE PRINCIPALE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸš€ INIZIO PROCESSO INGESTIONE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_total = time.time()\n",
        "\n",
        "try:\n",
        "    # Fase 1: Dolphin OCR\n",
        "    dolphin_success = run_batch_dolphin_ocr()\n",
        "\n",
        "    if not dolphin_success:\n",
        "        raise RuntimeError(\"Dolphin OCR fallito\")\n",
        "\n",
        "    # Fase 2: Text DB\n",
        "    text_db_success = create_text_db()\n",
        "\n",
        "    if not text_db_success:\n",
        "        raise RuntimeError(\"Text DB creation fallito\")\n",
        "\n",
        "    # Fase 3: Image DB + CLIP\n",
        "    image_db_success = create_image_db_with_selective_clip()\n",
        "\n",
        "    if not image_db_success:\n",
        "        raise RuntimeError(\"Image DB creation fallito\")\n",
        "\n",
        "    elapsed_total = time.time() - start_total\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸŽ‰ PROCESSO INGESTIONE COMPLETATO!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nâ±ï¸ Tempo totale: {elapsed_total/60:.1f} minuti\")\n",
        "    print(f\"\\nðŸ“Š Output:\")\n",
        "    print(f\"   â€¢ Text DB: {DRIVE_TEXT_DB}\")\n",
        "    print(f\"   â€¢ Image DB: {DRIVE_IMAGE_DB}\")\n",
        "    print(f\"   â€¢ CLIP JSON: {DRIVE_CLIP_JSON}\")\n",
        "\n",
        "    # Verifica finale\n",
        "    print(f\"\\nðŸ” Verifica finale:\")\n",
        "    if os.path.exists(DRIVE_TEXT_DB):\n",
        "        print(f\"   âœ… Text DB creato correttamente\")\n",
        "    else:\n",
        "        print(f\"   âŒ Text DB NON trovato!\")\n",
        "\n",
        "    if os.path.exists(DRIVE_IMAGE_DB):\n",
        "        print(f\"   âœ… Image DB creato correttamente\")\n",
        "    else:\n",
        "        print(f\"   âŒ Image DB NON trovato!\")\n",
        "\n",
        "    if os.path.exists(DRIVE_CLIP_JSON):\n",
        "        clip_size = os.path.getsize(DRIVE_CLIP_JSON) / (1024 * 1024)\n",
        "        print(f\"   âœ… CLIP JSON creato correttamente ({clip_size:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸ CLIP JSON NON trovato (normale se nessuna immagine qualificata)\")\n",
        "\n",
        "    print(\"\\nâœ… Ora puoi procedere con i BLOCCHI 2, 3 o 4!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ ERRORE durante ingestione: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    print(\"\\nðŸ”§ Possibili soluzioni:\")\n",
        "    print(\"   1. Verifica che Dolphin sia installato (BLOCCO 0)\")\n",
        "    print(\"   2. Verifica connessione GPU\")\n",
        "    print(\"   3. Verifica spazio su Drive (>5GB liberi)\")\n",
        "    print(\"   4. Controlla log errori sopra\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70) '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGYT_jWtPyjp"
      },
      "source": [
        "#Evaluation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjEuKzishn6u"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 2: EVALUATION & TESTING FRAMEWORK (v3.0 - AUTO-UPDATE REPORT)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ðŸš€ BLOCCO 2: Evaluation & Testing Framework (v3.0 - AUTO-UPDATE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# SETUP DIRECTORIES SU DRIVE\n",
        "# ==========================================\n",
        "\n",
        "class PathConfig:\n",
        "    \"\"\"Configurazione path persistenti su Drive\"\"\"\n",
        "\n",
        "    DRIVE_BASE = Path(\"/content/drive/MyDrive/OCR\")\n",
        "    EVALUATION_BASE = DRIVE_BASE / \"evaluation\"\n",
        "\n",
        "    # Subdirectories\n",
        "    LOGS_DIR = EVALUATION_BASE / \"logs\"\n",
        "    REPORTS_DIR = EVALUATION_BASE / \"reports\"\n",
        "    INJECTION_TESTS_DIR = EVALUATION_BASE / \"injection_tests\"\n",
        "\n",
        "    # Files\n",
        "    QUERIES_LOG = LOGS_DIR / \"queries.jsonl\"\n",
        "    SUMMARY_LOG = LOGS_DIR / \"metrics_summary.json\"\n",
        "    INJECTION_RESULTS = INJECTION_TESTS_DIR / \"injection_results.json\"\n",
        "    EVALUATION_REPORT = REPORTS_DIR / \"automate_evaluation_report.md\"\n",
        "\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        \"\"\"Crea tutte le directory necessarie\"\"\"\n",
        "        for dir_path in [cls.LOGS_DIR, cls.REPORTS_DIR, cls.INJECTION_TESTS_DIR]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"âœ… Directory create in: {cls.EVALUATION_BASE}\")\n",
        "\n",
        "# Setup iniziale\n",
        "PathConfig.setup_directories()\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ“ Struttura creata:\n",
        "   {PathConfig.EVALUATION_BASE}/\n",
        "   â”œâ”€â”€ logs/\n",
        "   â”‚   â”œâ”€â”€ queries.jsonl\n",
        "   â”‚   â””â”€â”€ metrics_summary.json\n",
        "   â”œâ”€â”€ reports/\n",
        "   â”‚   â””â”€â”€ automate_evaluation_report.md (auto-updated)\n",
        "   â””â”€â”€ injection_tests/\n",
        "       â””â”€â”€ injection_results.json\n",
        "\"\"\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIDENCE CALCULATOR\n",
        "# ==========================================\n",
        "\n",
        "class ConfidenceCalculator:\n",
        "    \"\"\"\n",
        "    Calcola confidence da 3 fonti indipendenti\n",
        "    Reference-free approach (no ground truth needed)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def retrieval_confidence(docs_with_scores: List[Tuple]) -> float:\n",
        "        \"\"\"\n",
        "        Score basato su similarity e consistency dei retrieved chunks\n",
        "\n",
        "        Args:\n",
        "            docs_with_scores: List of (Document, distance_score) tuples\n",
        "\n",
        "        Returns:\n",
        "            float: Confidence score [0, 1]\n",
        "        \"\"\"\n",
        "        if not docs_with_scores:\n",
        "            return 0.0\n",
        "\n",
        "        # Convert distance to similarity (Chroma usa L2 distance)\n",
        "        scores = [1 - min(score, 1.0) for _, score in docs_with_scores]\n",
        "\n",
        "        # Componente 1: Similarity media\n",
        "        avg_similarity = np.mean(scores)\n",
        "\n",
        "        # Componente 2: Consistency (bassa varianza = alta consistency)\n",
        "        consistency = 1 - np.std(scores) if len(scores) > 1 else 1.0\n",
        "\n",
        "        # Componente 3: PenalitÃ  per troppa diversitÃ  di manuali\n",
        "        manuals = set(doc.metadata.get('manual', 'unknown') for doc, _ in docs_with_scores)\n",
        "        diversity_penalty = 0.95 if len(manuals) > 2 else 1.0\n",
        "\n",
        "        # Weighted combination\n",
        "        confidence = (avg_similarity * 0.6 + consistency * 0.4) * diversity_penalty\n",
        "\n",
        "        return round(confidence, 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def context_relevance(query: str, chunks: List, embed_model) -> float:\n",
        "        \"\"\"\n",
        "        Semantic similarity tra query e retrieved context\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "            chunks: List of Document objects\n",
        "            embed_model: HuggingFaceEmbeddings model\n",
        "\n",
        "        Returns:\n",
        "            float: Relevance score [0, 1]\n",
        "        \"\"\"\n",
        "        if not chunks:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            # Query embedding\n",
        "            query_emb = embed_model.embed_query(query)\n",
        "            query_emb = np.array(query_emb)\n",
        "\n",
        "            relevances = []\n",
        "            for chunk in chunks:\n",
        "                # Chunk embedding (primi 500 char per efficienza)\n",
        "                chunk_text = chunk.page_content[:500]\n",
        "                chunk_emb = embed_model.embed_query(chunk_text)\n",
        "                chunk_emb = np.array(chunk_emb)\n",
        "\n",
        "                # Cosine similarity\n",
        "                similarity = np.dot(query_emb, chunk_emb) / (\n",
        "                    np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb)\n",
        "                )\n",
        "                relevances.append(similarity)\n",
        "\n",
        "            return round(np.mean(relevances), 3)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Context relevance calculation failed: {e}\")\n",
        "            return 0.5  # Neutral fallback\n",
        "\n",
        "    @staticmethod\n",
        "    def answer_quality(response: str) -> float:\n",
        "        \"\"\"\n",
        "        Heuristics per valutare qualitÃ  della risposta\n",
        "\n",
        "        Args:\n",
        "            response: Generated answer string\n",
        "\n",
        "        Returns:\n",
        "            float: Quality score [0, 1]\n",
        "        \"\"\"\n",
        "        if not response or len(response.strip()) < 10:\n",
        "            return 0.0\n",
        "\n",
        "        score = 0.0\n",
        "        words = response.split()\n",
        "        length = len(words)\n",
        "\n",
        "        # 1. Lunghezza ottimale (35% peso)\n",
        "        if 50 < length < 300:\n",
        "            score += 0.35\n",
        "        elif 30 < length <= 50:\n",
        "            score += 0.20\n",
        "        elif length >= 300:\n",
        "            score += 0.15\n",
        "\n",
        "        # 2. Dettagli tecnici (25% peso)\n",
        "        # Numeri con unitÃ  di misura\n",
        "        if re.search(r'\\d+\\s*(km|m|cm|mm|kg|g|Â°C|bar|V|A|kW|rpm|l|ml)', response):\n",
        "            score += 0.15\n",
        "\n",
        "        # Keywords automotive\n",
        "        automotive_keywords = ['premere', 'ruotare', 'spegnere', 'accendere',\n",
        "                               'verificare', 'controllare', 'impostare', 'selezionare',\n",
        "                               'mantenere', 'rilasciare', 'press', 'turn', 'switch']\n",
        "        if any(kw in response.lower() for kw in automotive_keywords):\n",
        "            score += 0.10\n",
        "\n",
        "        # 3. Struttura articolata (20% peso)\n",
        "        sentences = [s.strip() for s in response.split('.') if len(s.strip()) > 10]\n",
        "        if len(sentences) >= 3:\n",
        "            score += 0.15\n",
        "        elif len(sentences) == 2:\n",
        "            score += 0.10\n",
        "        else:\n",
        "            score += 0.05\n",
        "\n",
        "        # 4. Markers di procedura (10% peso)\n",
        "        procedure_markers = ['primo', 'secondo', 'infine', 'successivamente',\n",
        "                             'quindi', 'passo', '1.', '2.', 'a)', 'b)', 'first', 'second']\n",
        "        if any(marker in response.lower() for marker in procedure_markers):\n",
        "            score += 0.10\n",
        "\n",
        "        # 5. Non Ã¨ risposta di fallback (10% peso)\n",
        "        fallback_phrases = ['non riesco', 'non posso', 'non ho informazioni',\n",
        "                            'non Ã¨ presente', 'mi scuso', 'cannot', 'unable']\n",
        "        if not any(phrase in response.lower() for phrase in fallback_phrases):\n",
        "            score += 0.10\n",
        "\n",
        "        return round(min(score, 1.0), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate(retrieval: float, relevance: float, quality: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Combina i 3 score in confidence finale\n",
        "\n",
        "        Args:\n",
        "            retrieval: Retrieval confidence [0, 1]\n",
        "            relevance: Context relevance [0, 1]\n",
        "            quality: Answer quality [0, 1]\n",
        "\n",
        "        Returns:\n",
        "            dict: {score, label, color, breakdown}\n",
        "        \"\"\"\n",
        "        # Weighted average (pesi ottimizzati per RAG)\n",
        "        weights = {\n",
        "            'retrieval': 0.40,   # QualitÃ  chunks retrieved\n",
        "            'relevance': 0.35,   # Rilevanza semantica\n",
        "            'quality': 0.25      # QualitÃ  risposta\n",
        "        }\n",
        "\n",
        "        final = (\n",
        "            retrieval * weights['retrieval'] +\n",
        "            relevance * weights['relevance'] +\n",
        "            quality * weights['quality']\n",
        "        )\n",
        "\n",
        "        # Classificazione per UI\n",
        "        if final >= 0.75:\n",
        "            label = \"HIGH\"\n",
        "            color = \"#4CAF50\"  # Green\n",
        "        elif final >= 0.55:\n",
        "            label = \"MEDIUM\"\n",
        "            color = \"#FF9800\"  # Orange\n",
        "        else:\n",
        "            label = \"LOW\"\n",
        "            color = \"#F44336\"  # Red\n",
        "\n",
        "        return {\n",
        "            'score': round(final, 3),\n",
        "            'label': label,\n",
        "            'color': color,\n",
        "            'breakdown': {\n",
        "                'retrieval': round(retrieval, 3),\n",
        "                'relevance': round(relevance, 3),\n",
        "                'quality': round(quality, 3)\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"âœ… ConfidenceCalculator loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. LLM-AS-JUDGE\n",
        "# ==========================================\n",
        "\n",
        "class LLMJudge:\n",
        "    \"\"\"\n",
        "    Usa Gemini come arbitro per valutazione qualitativa\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            llm: ChatGoogleGenerativeAI instance\n",
        "        \"\"\"\n",
        "        self.llm = llm\n",
        "\n",
        "    def evaluate_response(self, query: str, context: str, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Valuta risposta su 3 dimensioni: Faithfulness, Relevance, Completeness\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            context: Retrieved context (concatenated chunks)\n",
        "            response: Generated answer\n",
        "\n",
        "        Returns:\n",
        "            dict: {faithfulness, relevance, completeness, average, reasoning}\n",
        "        \"\"\"\n",
        "\n",
        "        # Truncate context se troppo lungo (Gemini limit)\n",
        "        max_context_chars = 2000\n",
        "        if len(context) > max_context_chars:\n",
        "            context = context[:max_context_chars] + \"\\n... [truncated]\"\n",
        "\n",
        "        judge_prompt = f\"\"\"Evaluate this RAG response (scale 1-5):\n",
        "\n",
        "CONTEXT (from manual):\n",
        "{context}\n",
        "\n",
        "USER QUESTION:\n",
        "{query}\n",
        "\n",
        "GENERATED ANSWER:\n",
        "{response}\n",
        "\n",
        "EVALUATE according to these 3 criteria (scale 1-5):\n",
        "\n",
        "1. **FAITHFULNESS** (Fidelity to context)\n",
        "   - 1: Invented or contradictory information\n",
        "   - 3: Partially supported by context\n",
        "   - 5: Completely faithful to context\n",
        "\n",
        "2. **RELEVANCE** (Pertinence to question)\n",
        "   - 1: Does not answer the question\n",
        "   - 3: Partial or tangential answer\n",
        "   - 5: Direct and complete answer\n",
        "\n",
        "3. **COMPLETENESS** (Technical detail)\n",
        "   - 1: Vague, missing essential details\n",
        "   - 3: Sufficient but not detailed\n",
        "   - 5: Very detailed with technical specs\n",
        "\n",
        "IMPORTANT: Respond ONLY with valid JSON (NO markdown, NO extra explanations):\n",
        "{{\n",
        "  \"faithfulness\": <number 1-5>,\n",
        "  \"relevance\": <number 1-5>,\n",
        "  \"completeness\": <number 1-5>,\n",
        "  \"reasoning\": \"<max 100 characters>\"\n",
        "}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Invoke LLM\n",
        "            judge_response = self.llm.invoke(judge_prompt)\n",
        "\n",
        "            # Extract content from AIMessage object\n",
        "            if hasattr(judge_response, 'content'):\n",
        "                json_str = judge_response.content\n",
        "            else:\n",
        "                json_str = str(judge_response)\n",
        "\n",
        "            json_str = json_str.strip()\n",
        "\n",
        "            # Rimuovi markdown wrapper se presente\n",
        "            json_str = re.sub(r'```json\\s*', '', json_str)\n",
        "            json_str = re.sub(r'```\\s*$', '', json_str)\n",
        "            json_str = re.sub(r'^```\\s*', '', json_str)\n",
        "\n",
        "            # Parse\n",
        "            scores = json.loads(json_str)\n",
        "\n",
        "            # Validazione\n",
        "            required_keys = ['faithfulness', 'relevance', 'completeness']\n",
        "            for key in required_keys:\n",
        "                if key not in scores:\n",
        "                    raise ValueError(f\"Missing key: {key}\")\n",
        "                if not isinstance(scores[key], (int, float)):\n",
        "                    raise ValueError(f\"Invalid type for {key}\")\n",
        "                if not (1 <= scores[key] <= 5):\n",
        "                    raise ValueError(f\"Score out of range for {key}: {scores[key]}\")\n",
        "\n",
        "            # Calcola average\n",
        "            scores['average'] = round(\n",
        "                (scores['faithfulness'] + scores['relevance'] + scores['completeness']) / 3,\n",
        "                2\n",
        "            )\n",
        "\n",
        "            # Assicura reasoning sia stringa\n",
        "            if 'reasoning' not in scores:\n",
        "                scores['reasoning'] = \"No reasoning provided\"\n",
        "\n",
        "            logger.info(f\"LLM Judge completed: avg={scores['average']}\")\n",
        "            return scores\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.error(f\"JSON parse failed: {e}\")\n",
        "            raw_preview = json_str[:200] if 'json_str' in locals() else \"N/A\"\n",
        "            logger.error(f\"Raw response preview: {raw_preview}\")\n",
        "            return {\n",
        "                'faithfulness': 0,\n",
        "                'relevance': 0,\n",
        "                'completeness': 0,\n",
        "                'average': 0,\n",
        "                'reasoning': f\"JSON parse error: {str(e)[:50]}\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"LLM Judge failed: {e}\")\n",
        "            return {\n",
        "                'faithfulness': 0,\n",
        "                'relevance': 0,\n",
        "                'completeness': 0,\n",
        "                'average': 0,\n",
        "                'reasoning': f\"Error: {str(e)[:50]}\"\n",
        "            }\n",
        "\n",
        "print(\"âœ… LLMJudge loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. RETRIEVAL METRICS\n",
        "# ==========================================\n",
        "\n",
        "class RetrievalMetrics:\n",
        "    \"\"\"\n",
        "    Metriche intrinseche di retrieval (no ground truth)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def diversity_score(retrieved_docs: List) -> float:\n",
        "        \"\"\"\n",
        "        DiversitÃ  manuali: quanti manuali diversi nei retrieved chunks?\n",
        "\n",
        "        Returns:\n",
        "            float: Ratio unique_manuals / total_docs [0, 1]\n",
        "        \"\"\"\n",
        "        if not retrieved_docs:\n",
        "            return 0.0\n",
        "\n",
        "        manuals = [doc.metadata.get('manual', 'unknown') for doc in retrieved_docs]\n",
        "        unique_manuals = len(set(manuals))\n",
        "\n",
        "        return round(unique_manuals / len(manuals), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_similarity_variance(scores: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Consistency: quanto sono simili tra loro i chunks?\n",
        "        Bassa varianza = chunks coerenti\n",
        "\n",
        "        Returns:\n",
        "            float: 1 - std(scores) [0, 1]\n",
        "        \"\"\"\n",
        "        if len(scores) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        return round(1 - np.std(scores), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def average_chunk_score(scores: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Score medio di similarity\n",
        "\n",
        "        Returns:\n",
        "            float: Mean similarity [0, 1]\n",
        "        \"\"\"\n",
        "        if not scores:\n",
        "            return 0.0\n",
        "\n",
        "        # Convert distance to similarity se necessario\n",
        "        similarities = [1 - min(s, 1.0) for s in scores]\n",
        "\n",
        "        return round(np.mean(similarities), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_all(retrieved_docs: List, scores: List[float]) -> Dict:\n",
        "        \"\"\"\n",
        "        Calcola tutte le metriche in un colpo\n",
        "\n",
        "        Returns:\n",
        "            dict: All retrieval metrics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'diversity': RetrievalMetrics.diversity_score(retrieved_docs),\n",
        "            'consistency': RetrievalMetrics.chunk_similarity_variance(scores),\n",
        "            'avg_similarity': RetrievalMetrics.average_chunk_score(scores),\n",
        "            'num_chunks': len(retrieved_docs)\n",
        "        }\n",
        "\n",
        "print(\"âœ… RetrievalMetrics loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. EVALUATION LOGGER (AUTO-UPDATE REPORT)\n",
        "# ==========================================\n",
        "\n",
        "@dataclass\n",
        "class QueryLogEntry:\n",
        "    \"\"\"Struttura log entry per JSONL\"\"\"\n",
        "    timestamp: str\n",
        "    query: str\n",
        "    response: str\n",
        "    vehicle: Optional[str]\n",
        "    confidence: Dict\n",
        "    llm_judge: Dict\n",
        "    retrieval_metrics: Dict\n",
        "    performance: Dict\n",
        "    clip_used: bool\n",
        "    injection_detected: bool\n",
        "    images_found: int\n",
        "    images_shown: int\n",
        "    language: str\n",
        "\n",
        "class EvaluationLogger:\n",
        "    \"\"\"\n",
        "    Logger persistente su Google Drive con AUTO-UPDATE del report\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.queries_log = PathConfig.QUERIES_LOG\n",
        "        self.summary_log = PathConfig.SUMMARY_LOG\n",
        "        self.report_path = PathConfig.EVALUATION_REPORT\n",
        "\n",
        "    def log_query(self, data: Dict):\n",
        "        \"\"\"\n",
        "        Appendi query al log JSONL e RIGENERA automaticamente il report\n",
        "\n",
        "        Args:\n",
        "            data: Dictionary con tutti i campi del log\n",
        "        \"\"\"\n",
        "        # Crea entry strutturata\n",
        "        entry = QueryLogEntry(\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "            query=data.get('query', ''),\n",
        "            response=data.get('response', ''),\n",
        "            vehicle=data.get('vehicle'),\n",
        "            confidence=data.get('confidence', {}),\n",
        "            llm_judge=data.get('llm_judge', {}),\n",
        "            retrieval_metrics=data.get('retrieval_metrics', {}),\n",
        "            performance=data.get('performance', {}),\n",
        "            clip_used=data.get('clip_used', False),\n",
        "            injection_detected=data.get('injection_detected', False),\n",
        "            images_found=data.get('images_found', 0),\n",
        "            images_shown=data.get('images_shown', 0),\n",
        "            language=data.get('language', 'unknown')\n",
        "        )\n",
        "\n",
        "        # Appendi a JSONL\n",
        "        with open(self.queries_log, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\\n')\n",
        "\n",
        "        logger.info(f\"Query logged: {data['query'][:50]}...\")\n",
        "\n",
        "        # âœ… AUTO-REGENERATE REPORT\n",
        "        self.generate_markdown_report()\n",
        "\n",
        "    def load_all_queries(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Carica tutte le query dal log\n",
        "\n",
        "        Returns:\n",
        "            List of query dictionaries\n",
        "        \"\"\"\n",
        "        if not self.queries_log.exists():\n",
        "            return []\n",
        "\n",
        "        queries = []\n",
        "        with open(self.queries_log, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    queries.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        return queries\n",
        "\n",
        "    def compute_summary(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Calcola statistiche aggregate da queries.jsonl\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics\n",
        "        \"\"\"\n",
        "        queries = self.load_all_queries()\n",
        "\n",
        "        if not queries:\n",
        "            return {\n",
        "                'total_queries': 0,\n",
        "                'message': 'No data collected yet'\n",
        "            }\n",
        "\n",
        "        # Estrai metriche\n",
        "        confidences = []\n",
        "        judge_scores = []\n",
        "        response_times = []\n",
        "        rag_times = []\n",
        "        llm_times = []\n",
        "        img_times = []\n",
        "        clip_used_count = 0\n",
        "        injection_count = 0\n",
        "        retrieval_metrics = {'diversity': [], 'consistency': [], 'avg_similarity': []}\n",
        "\n",
        "        for q in queries:\n",
        "            # Confidence\n",
        "            conf = q.get('confidence', {})\n",
        "            if 'score' in conf:\n",
        "                confidences.append(conf['score'])\n",
        "\n",
        "            # LLM Judge\n",
        "            judge = q.get('llm_judge', {})\n",
        "            if 'average' in judge and judge['average'] > 0:\n",
        "                judge_scores.append(judge['average'])\n",
        "\n",
        "            # Performance\n",
        "            perf = q.get('performance', {})\n",
        "            response_times.append(perf.get('total_time_ms', 0))\n",
        "            rag_times.append(perf.get('rag_time_ms', 0))\n",
        "            llm_times.append(perf.get('llm_time_ms', 0))\n",
        "            img_times.append(perf.get('img_time_ms', 0))\n",
        "\n",
        "            # CLIP\n",
        "            if q.get('clip_used', False):\n",
        "                clip_used_count += 1\n",
        "\n",
        "            # Security\n",
        "            if q.get('injection_detected', False):\n",
        "                injection_count += 1\n",
        "\n",
        "            # Retrieval\n",
        "            ret_metrics = q.get('retrieval_metrics', {})\n",
        "            for key in retrieval_metrics:\n",
        "                if key in ret_metrics:\n",
        "                    retrieval_metrics[key].append(ret_metrics[key])\n",
        "\n",
        "        # Aggregate\n",
        "        summary = {\n",
        "            'total_queries': len(queries),\n",
        "            'date_range': {\n",
        "                'start': queries[0]['timestamp'],\n",
        "                'end': queries[-1]['timestamp']\n",
        "            },\n",
        "            'confidence': {\n",
        "                'mean': round(np.mean(confidences), 3) if confidences else 0,\n",
        "                'std': round(np.std(confidences), 3) if confidences else 0,\n",
        "                'min': round(min(confidences), 3) if confidences else 0,\n",
        "                'max': round(max(confidences), 3) if confidences else 0,\n",
        "                'samples': len(confidences)\n",
        "            },\n",
        "            'llm_judge': {\n",
        "                'mean': round(np.mean(judge_scores), 3) if judge_scores else 0,\n",
        "                'std': round(np.std(judge_scores), 3) if judge_scores else 0,\n",
        "                'samples': len(judge_scores)\n",
        "            },\n",
        "            'retrieval': {\n",
        "                'diversity': {\n",
        "                    'mean': round(np.mean(retrieval_metrics['diversity']), 3) if retrieval_metrics['diversity'] else 0,\n",
        "                    'std': round(np.std(retrieval_metrics['diversity']), 3) if retrieval_metrics['diversity'] else 0\n",
        "                },\n",
        "                'consistency': {\n",
        "                    'mean': round(np.mean(retrieval_metrics['consistency']), 3) if retrieval_metrics['consistency'] else 0,\n",
        "                    'std': round(np.std(retrieval_metrics['consistency']), 3) if retrieval_metrics['consistency'] else 0\n",
        "                },\n",
        "                'avg_similarity': {\n",
        "                    'mean': round(np.mean(retrieval_metrics['avg_similarity']), 3) if retrieval_metrics['avg_similarity'] else 0,\n",
        "                    'std': round(np.std(retrieval_metrics['avg_similarity']), 3) if retrieval_metrics['avg_similarity'] else 0\n",
        "                }\n",
        "            },\n",
        "            'performance': {\n",
        "                'avg_total_time_ms': round(np.mean(response_times), 1) if response_times else 0,\n",
        "                'avg_rag_time_ms': round(np.mean(rag_times), 1) if rag_times else 0,\n",
        "                'avg_llm_time_ms': round(np.mean(llm_times), 1) if llm_times else 0,\n",
        "                'avg_img_time_ms': round(np.mean(img_times), 1) if img_times else 0,\n",
        "                'clip_usage_rate': round(clip_used_count / len(queries), 3) if queries else 0\n",
        "            },\n",
        "            'security': {\n",
        "                'injection_attempts': injection_count,\n",
        "                'block_rate': round(injection_count / len(queries), 3) if queries else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Salva summary\n",
        "        with open(self.summary_log, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def generate_markdown_report(self) -> str:\n",
        "        \"\"\"\n",
        "        Genera report Markdown AUTO-AGGIORNATO con:\n",
        "        - Stats generali aggregate\n",
        "        - Last 10 queries dettagliate\n",
        "\n",
        "        Returns:\n",
        "            str: Path del report\n",
        "        \"\"\"\n",
        "        queries = self.load_all_queries()\n",
        "\n",
        "        if not queries:\n",
        "            report = \"# AutoMATE Evaluation Report\\n\\n**No data collected yet.**\\n\\nRun at least 1 query to generate the report.\"\n",
        "            with open(self.report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(report)\n",
        "            return str(self.report_path)\n",
        "\n",
        "        summary = self.compute_summary()\n",
        "\n",
        "        # Get last 10 queries\n",
        "        last_10 = queries[-10:]\n",
        "\n",
        "        report = f\"\"\"# AutoMATE Evaluation Report\n",
        "**Automotive Multimodal Augmented Technical Expert**\n",
        "\n",
        "*Auto-generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Executive Summary\n",
        "\n",
        "- **Total Queries**: {summary['total_queries']}\n",
        "- **Period**: {summary['date_range']['start'][:10]} to {summary['date_range']['end'][:10]}\n",
        "- **System**: Dolphin OCR + Gemini 2.0 Flash + CLIP ViT-B/32\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Aggregate Performance Metrics\n",
        "\n",
        "### Confidence Scores (Multi-source)\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Mean Confidence** | {summary['confidence']['mean']} Â± {summary['confidence']['std']} |\n",
        "| **Range** | [{summary['confidence']['min']}, {summary['confidence']['max']}] |\n",
        "| **Samples** | {summary['confidence']['samples']}/{summary['total_queries']} |\n",
        "\n",
        "**Interpretation:**\n",
        "- `â‰¥ 0.75`: **HIGH** confidence (system very confident in answer)\n",
        "- `0.55-0.75`: **MEDIUM** confidence (reliable answer with caution)\n",
        "- `< 0.55`: **LOW** confidence (uncertain answer, requires manual verification)\n",
        "\n",
        "### LLM-as-Judge Evaluation\n",
        "\n",
        "| Dimension | Score |\n",
        "|-----------|-------|\n",
        "| **Average Score** | {summary['llm_judge']['mean']}/5.0 Â± {summary['llm_judge']['std']} |\n",
        "| **Samples** | {summary['llm_judge']['samples']}/{summary['total_queries']} |\n",
        "\n",
        "### Retrieval Performance\n",
        "\n",
        "| Metric | Mean Â± Std |\n",
        "|--------|-----------|\n",
        "| **Diversity** | {summary['retrieval']['diversity']['mean']} Â± {summary['retrieval']['diversity']['std']} |\n",
        "| **Consistency** | {summary['retrieval']['consistency']['mean']} Â± {summary['retrieval']['consistency']['std']} |\n",
        "| **Avg Similarity** | {summary['retrieval']['avg_similarity']['mean']} Â± {summary['retrieval']['avg_similarity']['std']} |\n",
        "\n",
        "### System Performance\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Avg Response Time** | {summary['performance']['avg_total_time_ms']:.0f}ms |\n",
        "| **Avg RAG Time** | {summary['performance']['avg_rag_time_ms']:.0f}ms |\n",
        "| **Avg LLM Time** | {summary['performance']['avg_llm_time_ms']:.0f}ms |\n",
        "| **Avg Image Time** | {summary['performance']['avg_img_time_ms']:.0f}ms |\n",
        "| **CLIP Usage Rate** | {summary['performance']['clip_usage_rate']*100:.1f}% |\n",
        "\n",
        "### Security & Safety\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Injection Attempts** | {summary['security']['injection_attempts']} |\n",
        "| **Block Rate** | {summary['security']['block_rate']*100:.1f}% |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Last 10 Queries (Detailed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Add detailed info for last 10 queries\n",
        "        for idx, q in enumerate(reversed(last_10), 1):\n",
        "            timestamp = q.get('timestamp', 'N/A')\n",
        "            query_text = q.get('query', 'N/A')\n",
        "            vehicle = q.get('vehicle', 'Generic')\n",
        "            language = q.get('language', 'unknown')\n",
        "\n",
        "            # Confidence\n",
        "            conf = q.get('confidence', {})\n",
        "            conf_score = conf.get('score', 0)\n",
        "            conf_label = conf.get('label', 'N/A')\n",
        "            conf_breakdown = conf.get('breakdown', {})\n",
        "\n",
        "            # LLM Judge\n",
        "            judge = q.get('llm_judge', {})\n",
        "            judge_avg = judge.get('average', 0)\n",
        "            judge_faith = judge.get('faithfulness', 0)\n",
        "            judge_rel = judge.get('relevance', 0)\n",
        "            judge_comp = judge.get('completeness', 0)\n",
        "            judge_reason = judge.get('reasoning', 'N/A')\n",
        "\n",
        "            # Retrieval metrics\n",
        "            ret = q.get('retrieval_metrics', {})\n",
        "            diversity = ret.get('diversity', 0)\n",
        "            consistency = ret.get('consistency', 0)\n",
        "            avg_sim = ret.get('avg_similarity', 0)\n",
        "            num_chunks = ret.get('num_chunks', 0)\n",
        "\n",
        "            # Performance\n",
        "            perf = q.get('performance', {})\n",
        "            total_time = perf.get('total_time_ms', 0)\n",
        "            rag_time = perf.get('rag_time_ms', 0)\n",
        "            llm_time = perf.get('llm_time_ms', 0)\n",
        "            img_time = perf.get('img_time_ms', 0)\n",
        "\n",
        "            # Images\n",
        "            img_found = q.get('images_found', 0)\n",
        "            img_shown = q.get('images_shown', 0)\n",
        "            clip_used = \"âœ…\" if q.get('clip_used', False) else \"âŒ\"\n",
        "\n",
        "            report += f\"\"\"### Query #{len(queries) - len(last_10) + idx}: {query_text[:80]}{\"...\" if len(query_text) > 80 else \"\"}\n",
        "\n",
        "**Timestamp:** {timestamp}\n",
        "**Vehicle:** {vehicle} | **Language:** {language}\n",
        "\n",
        "#### ðŸŽ¯ Confidence Score: {conf_label} ({conf_score})\n",
        "\n",
        "| Component | Score |\n",
        "|-----------|-------|\n",
        "| Retrieval | {conf_breakdown.get('retrieval', 0)} |\n",
        "| Relevance | {conf_breakdown.get('relevance', 0)} |\n",
        "| Quality | {conf_breakdown.get('quality', 0)} |\n",
        "\n",
        "#### ðŸ“š Retrieval Metrics\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Diversity | {diversity} |\n",
        "| Consistency | {consistency} |\n",
        "| Avg Similarity | {avg_sim} |\n",
        "| Chunks Retrieved | {num_chunks} |\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            # Add LLM Judge if available\n",
        "            if judge_avg > 0:\n",
        "                report += f\"\"\"#### ðŸ¤– LLM Judge Evaluation: {judge_avg}/5.0\n",
        "\n",
        "| Dimension | Score |\n",
        "|-----------|-------|\n",
        "| Faithfulness | {judge_faith}/5 â­ |\n",
        "| Relevance | {judge_rel}/5 â­ |\n",
        "| Completeness | {judge_comp}/5 â­ |\n",
        "\n",
        "**Reasoning:** {judge_reason}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            report += f\"\"\"#### â±ï¸ Performance\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Total Time | {total_time:.0f}ms |\n",
        "| RAG Time | {rag_time:.0f}ms |\n",
        "| LLM Time | {llm_time:.0f}ms |\n",
        "| Image Time | {img_time:.0f}ms |\n",
        "\n",
        "#### ðŸ–¼ï¸ Images\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Found | {img_found} |\n",
        "| Shown | {img_shown} |\n",
        "| CLIP Used | {clip_used} |\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "*Report auto-generated by AutoMATE Evaluation Framework v3.0*\n",
        "\n",
        "*Full query log: `{self.queries_log}`*\n",
        "\"\"\"\n",
        "\n",
        "        # Save report\n",
        "        with open(self.report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        logger.info(f\"Report auto-updated: {self.report_path}\")\n",
        "\n",
        "        return str(self.report_path)\n",
        "\n",
        "print(\"âœ… EvaluationLogger loaded (AUTO-UPDATE)\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. PROMPT PROTECTION\n",
        "# ==========================================\n",
        "\n",
        "class PromptProtection:\n",
        "    \"\"\"Security contro injection attacks\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_query(query: str) -> Tuple[str, bool]:\n",
        "        original = query\n",
        "        patterns = [\n",
        "            (r'<\\|[^>]*\\|>', ''),\n",
        "            (r'system\\s*:', '', re.IGNORECASE),\n",
        "            (r'ignore\\s+(previous|all|above|instructions?)', '', re.IGNORECASE),\n",
        "            (r'forget\\s+(previous|all|above|instructions?)', '', re.IGNORECASE),\n",
        "            (r'you\\s+are\\s+now', '', re.IGNORECASE),\n",
        "            (r'act\\s+as\\s+if', '', re.IGNORECASE),\n",
        "        ]\n",
        "        cleaned = query\n",
        "        for pattern, replacement, *flags in patterns:\n",
        "            flag = flags[0] if flags else 0\n",
        "            cleaned = re.sub(pattern, replacement, cleaned, flags=flag)\n",
        "        cleaned = ' '.join(cleaned.split()).strip()\n",
        "        return cleaned, (cleaned != original)\n",
        "\n",
        "print(\"âœ… PromptProtection loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# SUMMARY\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ‰ BLOCCO 2 COMPLETATO (v3.0 - AUTO-UPDATE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "âœ… Componenti:\n",
        "   1. ConfidenceCalculator - Multi-source confidence scoring\n",
        "   2. LLMJudge - Gemini as arbitrator (AIMessage compatible)\n",
        "   3. RetrievalMetrics - Intrinsic evaluation (no ground truth)\n",
        "   4. EvaluationLogger - AUTO-UPDATE report after each query\n",
        "   5. PromptProtection - Security test suite\n",
        "\n",
        "ðŸ“Š Report Features:\n",
        "   â€¢ Stats generali aggregate (dall'inizio)\n",
        "   â€¢ Last 10 queries con TUTTE le metriche\n",
        "   â€¢ Auto-rigenerato dopo OGNI query\n",
        "\n",
        "ðŸ“ Files su Drive:\n",
        "   {PathConfig.EVALUATION_BASE}/\n",
        "   â”œâ”€â”€ logs/queries.jsonl (log completo)\n",
        "   â”œâ”€â”€ logs/metrics_summary.json (stats aggregate)\n",
        "   â””â”€â”€ reports/automate_evaluation_report.md (AUTO-UPDATED!)\n",
        "\n",
        "ðŸ’¡ Il report si aggiorna automaticamente dopo ogni query!\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljvjGbBpP5R2"
      },
      "source": [
        "#Advanced Rag Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7SkXurUJ9-D"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 2.5: ADVANCED RAG COMPONENTS (FIXED v3.0)\n",
        "# ==========================================\n",
        "\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ðŸš€ BLOCCO 2.5: Advanced RAG Components (FIXED v3.0)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA BLOCCO 1\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ” Verifica prerequisiti...\")\n",
        "\n",
        "try:\n",
        "    _test_config = IMAGE_QUALITY_CONFIG\n",
        "    print(f\"âœ… IMAGE_QUALITY_CONFIG trovato\")\n",
        "    print(f\"   Threshold: {IMAGE_QUALITY_CONFIG['min_width']}x{IMAGE_QUALITY_CONFIG['min_height']}px, \"\n",
        "          f\"{IMAGE_QUALITY_CONFIG['min_size_kb']}KB, ratio<{IMAGE_QUALITY_CONFIG['max_aspect_ratio']}\")\n",
        "except NameError:\n",
        "    print(\"âŒ ERRORE: BLOCCO 1 (Config) non eseguito!\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# VEHICLE DETECTOR\n",
        "# ==========================================\n",
        "\n",
        "class VehicleDetector:\n",
        "    \"\"\"Named Entity Recognition per veicoli\"\"\"\n",
        "\n",
        "    VEHICLE_MAPPING = {\n",
        "        'peugeot 208': 'PEUGEOT 208',\n",
        "        '208': 'PEUGEOT 208',\n",
        "        'peugeot': 'PEUGEOT 208',\n",
        "        'fiat 500': '500',\n",
        "        '500': '500',\n",
        "        'cinquecento': '500',\n",
        "        'fiat cinquecento': '500',\n",
        "        'panda': 'PANDA',\n",
        "        'fiat panda': 'PANDA',\n",
        "        'grande punto': 'GRANDE-PUNTO',\n",
        "        'punto': 'GRANDE-PUNTO',\n",
        "        'fiat punto': 'GRANDE-PUNTO',\n",
        "        'fiat grande punto': 'GRANDE-PUNTO',\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def detect(cls, query: str) -> Optional[str]:\n",
        "        query_lower = query.lower()\n",
        "        sorted_variants = sorted(cls.VEHICLE_MAPPING.keys(), key=len, reverse=True)\n",
        "        for variant in sorted_variants:\n",
        "            if variant in query_lower:\n",
        "                manual_name = cls.VEHICLE_MAPPING[variant]\n",
        "                logger.info(f\"Vehicle detected: '{variant}' â†’ {manual_name}\")\n",
        "                return manual_name\n",
        "        return None\n",
        "\n",
        "    @classmethod\n",
        "    def get_all_manuals(cls) -> List[str]:\n",
        "        return list(set(cls.VEHICLE_MAPPING.values()))\n",
        "\n",
        "print(\"âœ… VehicleDetector loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# IMAGE QUALITY FILTER\n",
        "# ==========================================\n",
        "\n",
        "@dataclass\n",
        "class ImageQualityThreshold:\n",
        "    \"\"\"Threshold per quality filtering - eredita da BLOCCO 1\"\"\"\n",
        "    min_width: int = IMAGE_QUALITY_CONFIG['min_width']\n",
        "    min_height: int = IMAGE_QUALITY_CONFIG['min_height']\n",
        "    min_size_kb: float = IMAGE_QUALITY_CONFIG['min_size_kb']\n",
        "    max_aspect_ratio: float = IMAGE_QUALITY_CONFIG['max_aspect_ratio']\n",
        "\n",
        "class ImageQualityFilter:\n",
        "    \"\"\"Post-processing per escludere immagini di bassa qualitÃ \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_quality_image(img_path: str,\n",
        "                         threshold: ImageQualityThreshold = None) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Verifica se immagine rispetta standard di qualitÃ \n",
        "\n",
        "        Returns:\n",
        "            (is_quality, reason)\n",
        "        \"\"\"\n",
        "        if threshold is None:\n",
        "            threshold = ImageQualityThreshold()\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(img_path):\n",
        "                return False, \"FILE_NOT_FOUND\"\n",
        "\n",
        "            size_kb = os.path.getsize(img_path) / 1024\n",
        "            if size_kb < threshold.min_size_kb:\n",
        "                return False, f\"TOO_SMALL_FILE ({size_kb:.1f}KB < {threshold.min_size_kb}KB)\"\n",
        "\n",
        "            from PIL import Image\n",
        "            with Image.open(img_path) as img:\n",
        "                width, height = img.size\n",
        "\n",
        "                if width < threshold.min_width or height < threshold.min_height:\n",
        "                    return False, f\"LOW_RESOLUTION ({width}x{height})\"\n",
        "\n",
        "                aspect_ratio = max(width, height) / min(width, height)\n",
        "                if aspect_ratio > threshold.max_aspect_ratio:\n",
        "                    return False, f\"BAD_ASPECT_RATIO ({aspect_ratio:.1f})\"\n",
        "\n",
        "            return True, \"OK\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"ERROR: {str(e)[:50]}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_results(img_results: List,\n",
        "                       threshold: ImageQualityThreshold = None) -> List:\n",
        "        \"\"\"Filtra lista di risultati immagini\"\"\"\n",
        "        if threshold is None:\n",
        "            threshold = ImageQualityThreshold()\n",
        "\n",
        "        filtered = []\n",
        "        rejected_count = 0\n",
        "\n",
        "        for result in img_results:\n",
        "            doc = result[0]\n",
        "            img_path = doc.metadata.get('image_path')\n",
        "            is_quality, reason = ImageQualityFilter.is_quality_image(img_path, threshold)\n",
        "\n",
        "            if is_quality:\n",
        "                filtered.append(result)\n",
        "            else:\n",
        "                rejected_count += 1\n",
        "                logger.debug(f\"Rejected: {reason}\")\n",
        "\n",
        "        if rejected_count > 0:\n",
        "            logger.info(f\"Quality filter: {len(filtered)}/{len(img_results)} passed\")\n",
        "\n",
        "        return filtered\n",
        "\n",
        "print(\"âœ… ImageQualityFilter loaded\")\n",
        "print(f\"   Threshold: {IMAGE_QUALITY_CONFIG['min_width']}x{IMAGE_QUALITY_CONFIG['min_height']}px, {IMAGE_QUALITY_CONFIG['min_size_kb']}KB\")\n",
        "\n",
        "# ==========================================\n",
        "# PROMPT PROTECTION\n",
        "# ==========================================\n",
        "\n",
        "class PromptProtection:\n",
        "    \"\"\"Security contro injection attacks\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_query(query: str) -> Tuple[str, bool]:\n",
        "        original = query\n",
        "        patterns = [\n",
        "            (r'<\\|[^>]*\\|>', ''),\n",
        "            (r'system\\s*:', '', re.IGNORECASE),\n",
        "            (r'ignore\\s+(previous|all|above|instructions?)', '', re.IGNORECASE),\n",
        "            (r'forget\\s+(previous|all|above|instructions?)', '', re.IGNORECASE),\n",
        "            (r'you\\s+are\\s+now', '', re.IGNORECASE),\n",
        "            (r'act\\s+as\\s+if', '', re.IGNORECASE),\n",
        "        ]\n",
        "        cleaned = query\n",
        "        for pattern, replacement, *flags in patterns:\n",
        "            flag = flags[0] if flags else 0\n",
        "            cleaned = re.sub(pattern, replacement, cleaned, flags=flag)\n",
        "        cleaned = ' '.join(cleaned.split()).strip()\n",
        "        return cleaned, (cleaned != original)\n",
        "\n",
        "print(\"âœ… PromptProtection loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# EVALUATION LOGGER\n",
        "# ==========================================\n",
        "\n",
        "class EvaluationLogger:\n",
        "    \"\"\"Logger per salvare risultati su Drive\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        from pathlib import Path\n",
        "        self.log_path = Path(\"/content/drive/MyDrive/OCR/evaluation/logs/queries.jsonl\")\n",
        "        self.report_path = Path(\"/content/drive/MyDrive/OCR/evaluation/reports/evaluation_report.md\")\n",
        "        self.log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def log_query(self, log_entry: Dict):\n",
        "        import json\n",
        "        with open(self.log_path, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(log_entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    def generate_markdown_report(self) -> str:\n",
        "        import json\n",
        "        from datetime import datetime\n",
        "\n",
        "        queries = []\n",
        "        if self.log_path.exists():\n",
        "            with open(self.log_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        queries.append(json.loads(line))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        with open(self.report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Evaluation Report\\n\\n\")\n",
        "            f.write(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "            f.write(f\"**Total Queries**: {len(queries)}\\n\")\n",
        "\n",
        "        return str(self.report_path)\n",
        "\n",
        "print(\"âœ… EvaluationLogger loaded\")\n",
        "\n",
        "eval_logger = EvaluationLogger()\n",
        "\n",
        "# ==========================================\n",
        "# VEHICLE AWARE RETRIEVER\n",
        "# ==========================================\n",
        "\n",
        "class VehicleAwareRetriever:\n",
        "    \"\"\"Wrapper per retrieval con filtering per manuale\"\"\"\n",
        "\n",
        "    def __init__(self, text_db, img_db):\n",
        "        self.text_db = text_db\n",
        "        self.img_db = img_db\n",
        "\n",
        "    def retrieve_text(self, query: str, k: int = 6,\n",
        "                      manual_filter: Optional[str] = None) -> List:\n",
        "        if manual_filter:\n",
        "            results = self.text_db.similarity_search_with_score(\n",
        "                query, k=k*3, filter={\"manual\": manual_filter}\n",
        "            )\n",
        "            return results[:k]\n",
        "        else:\n",
        "            return self.text_db.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    def retrieve_images(self, query: str, stage1_k: int = 20,\n",
        "                        manual_filter: Optional[str] = None) -> List:\n",
        "        if manual_filter:\n",
        "            results = self.img_db.similarity_search_with_score(\n",
        "                query, k=stage1_k*3, filter={\"manual\": manual_filter}\n",
        "            )\n",
        "            return results[:stage1_k]\n",
        "        else:\n",
        "            return self.img_db.similarity_search_with_score(query, k=stage1_k)\n",
        "\n",
        "print(\"âœ… VehicleAwareRetriever loaded\")\n",
        "\n",
        "# ==========================================\n",
        "# SUMMARY\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ‰ BLOCCO 2.5 COMPLETATO (FIXED v3.0)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "âœ… Componenti:\n",
        "   1. VehicleDetector - NER per veicoli\n",
        "   2. ImageQualityFilter - Filtra immagini spurie\n",
        "   3. PromptProtection - Security\n",
        "   4. EvaluationLogger - Logging su Drive\n",
        "   5. VehicleAwareRetriever - Retrieval filtrato\n",
        "\n",
        "ðŸ“Š Threshold attuali (da BLOCCO 1):\n",
        "   â€¢ Resolution: {IMAGE_QUALITY_CONFIG['min_width']}x{IMAGE_QUALITY_CONFIG['min_height']}px\n",
        "   â€¢ Size: {IMAGE_QUALITY_CONFIG['min_size_kb']}KB\n",
        "   â€¢ Aspect ratio: <{IMAGE_QUALITY_CONFIG['max_aspect_ratio']}\n",
        "\n",
        "ðŸ’¡ Per modificare threshold: cambia BLOCCO 1\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5dVpOgP-Sr"
      },
      "source": [
        "#Test RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y12zsAoDht8o"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 3: TEST RAG SYSTEM (FIXED + IMAGE QUERY AUGMENTATION)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ðŸš€ BLOCCO 3: Test RAG System (FIXED + IMAGE QUERY AUGMENTATION)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA PREREQUISITI\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ” Verifica prerequisiti...\")\n",
        "\n",
        "try:\n",
        "    _test = IMAGE_QUALITY_CONFIG\n",
        "    print(\"âœ… BLOCCO 1 (Config)\")\n",
        "except NameError:\n",
        "    print(\"âŒ BLOCCO 1 non eseguito!\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    _test = ConfidenceCalculator\n",
        "    print(\"âœ… BLOCCO 2 (Evaluation)\")\n",
        "except NameError:\n",
        "    print(\"âŒ BLOCCO 2 non eseguito!\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    _test = VehicleDetector\n",
        "    print(\"âœ… BLOCCO 2.5 (Advanced RAG)\")\n",
        "except NameError:\n",
        "    print(\"âŒ BLOCCO 2.5 non eseguito!\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURAZIONE TEST\n",
        "# ==========================================\n",
        "\n",
        "QUERY = \"Come funziona il pulsante TRIP della fiat 500?\"\n",
        "\n",
        "ENABLE_LLM_JUDGE = True\n",
        "ENABLE_INJECTION_TEST = True\n",
        "\n",
        "# âœ… NEW: Query augmentation per immagini\n",
        "ENABLE_IMAGE_QUERY_AUGMENTATION = True\n",
        "\n",
        "quality_threshold = ImageQualityThreshold()\n",
        "\n",
        "print(f\"\"\"\n",
        "{\"=\"*70}\n",
        "âš™ï¸ CONFIGURAZIONE TEST (v3.2 - IMAGE QUERY AUGMENTATION):\n",
        "{\"=\"*70}\n",
        "\n",
        "ðŸ“ Query: \"{QUERY}\"\n",
        "\n",
        "ðŸ–¼ï¸ Quality Filter (da BLOCCO 1):\n",
        "   â€¢ Resolution: {quality_threshold.min_width}x{quality_threshold.min_height}px\n",
        "   â€¢ Size: {quality_threshold.min_size_kb}KB\n",
        "\n",
        "ðŸ”¬ Evaluation:\n",
        "   â€¢ LLM Judge: {'âœ… ENABLED' if ENABLE_LLM_JUDGE else 'âŒ DISABLED'}\n",
        "\n",
        "ðŸ†• Image Query Augmentation: {'âœ… ENABLED' if ENABLE_IMAGE_QUERY_AUGMENTATION else 'âŒ DISABLED'}\n",
        "   â†’ Espande la query per trovare immagini piÃ¹ rilevanti\n",
        "{\"=\"*70}\n",
        "\"\"\")\n",
        "\n",
        "# ==========================================\n",
        "# CARICAMENTO RISORSE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nâš™ï¸ CARICAMENTO RISORSE...\")\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "try:\n",
        "    from langchain_chroma import Chroma\n",
        "except ImportError:\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "\n",
        "print(\"  1ï¸âƒ£ Text Embeddings...\")\n",
        "text_embed = HuggingFaceEmbeddings(\n",
        "    model_name=TEXT_EMBED_MODEL,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "print(f\"     âœ… {TEXT_EMBED_MODEL}\")\n",
        "\n",
        "print(\"  2ï¸âƒ£ Vector Stores...\")\n",
        "text_db = Chroma(\n",
        "    collection_name=TEXT_COLLECTION_NAME,\n",
        "    persist_directory=CHROMA_TEXT_DIR,\n",
        "    embedding_function=text_embed\n",
        ")\n",
        "print(f\"     âœ… Text: {text_db._collection.count()} documenti\")\n",
        "\n",
        "img_db = Chroma(\n",
        "    collection_name=IMAGE_COLLECTION_NAME,\n",
        "    persist_directory=CHROMA_IMAGE_DIR,\n",
        "    embedding_function=text_embed\n",
        ")\n",
        "print(f\"     âœ… Images: {img_db._collection.count()} immagini\")\n",
        "\n",
        "print(\"  3ï¸âƒ£ CLIP...\")\n",
        "with open(CLIP_JSON_PATH, 'r') as f:\n",
        "    clip_embeddings = json.load(f)\n",
        "print(f\"     âœ… CLIP embeddings: {len(clip_embeddings)}\")\n",
        "\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to('cuda')\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "print(f\"     âœ… CLIP model: {CLIP_MODEL_NAME}\")\n",
        "\n",
        "print(\"  4ï¸âƒ£ Gemini LLM...\")\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, HarmCategory, HarmBlockThreshold\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        ")\n",
        "print(\"     âœ… Gemini 2.0 Flash Lite\")\n",
        "\n",
        "print(\"\\nâœ… Risorse caricate!\")\n",
        "\n",
        "# ==========================================\n",
        "# QUERY AUGMENTATION FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def augment_query_for_images(query: str, llm_instance) -> str:\n",
        "    \"\"\"\n",
        "    Usa Gemini per espandere la query con termini visivi/tecnici\n",
        "    che aiutano a trovare immagini rilevanti\n",
        "    \"\"\"\n",
        "    augment_prompt = f\"\"\"Data questa domanda su un manuale auto:\n",
        "\"{query}\"\n",
        "\n",
        "Genera una lista di 5-8 parole chiave che potrebbero apparire nella didascalia o nel contesto di un'immagine rilevante.\n",
        "Includi: componenti visibili, posizioni (cruscotto, volante, display), azioni (premere, visualizzare).\n",
        "\n",
        "Rispondi SOLO con le parole chiave separate da spazi, senza punteggiatura o spiegazioni.\n",
        "Esempio: pulsante cruscotto display visualizzazione reset azzeramento\"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm_instance.invoke(augment_prompt)\n",
        "        keywords = result.content if hasattr(result, 'content') else str(result)\n",
        "        keywords = keywords.strip().replace('\\n', ' ').replace(',', ' ')\n",
        "\n",
        "        # Combina query originale + keywords\n",
        "        augmented = f\"{query} {keywords}\"\n",
        "        return augmented\n",
        "    except Exception as e:\n",
        "        print(f\"     âš ï¸ Augmentation fallita: {e}\")\n",
        "        return query\n",
        "\n",
        "# ==========================================\n",
        "# SECURITY CHECK\n",
        "# ==========================================\n",
        "\n",
        "if ENABLE_INJECTION_TEST:\n",
        "    sanitized_query, was_modified = PromptProtection.sanitize_query(QUERY)\n",
        "    if was_modified:\n",
        "        print(f\"\\nâš ï¸ Query sanitizzata: {sanitized_query}\")\n",
        "    else:\n",
        "        print(\"\\nâœ… Query pulita\")\n",
        "else:\n",
        "    sanitized_query = QUERY\n",
        "\n",
        "# ==========================================\n",
        "# VEHICLE DETECTION\n",
        "# ==========================================\n",
        "\n",
        "detected_vehicle = VehicleDetector.detect(sanitized_query)\n",
        "\n",
        "print(f\"\\nðŸš— VEHICLE DETECTION\")\n",
        "print(\"-\"*70)\n",
        "if detected_vehicle:\n",
        "    print(f\"âœ… Veicolo rilevato: {detected_vehicle}\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ Nessun veicolo (query generica)\")\n",
        "\n",
        "# ==========================================\n",
        "# TEXT RAG\n",
        "# ==========================================\n",
        "\n",
        "print(f\"\\nðŸ¤– ELABORAZIONE RAG TESTUALE\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Retrieval\n",
        "print(\"ðŸ“š Retrieval chunks...\")\n",
        "if detected_vehicle:\n",
        "    docs_with_scores = text_db.similarity_search_with_score(\n",
        "        sanitized_query, k=18, filter={\"manual\": detected_vehicle}\n",
        "    )[:6]\n",
        "else:\n",
        "    docs_with_scores = text_db.similarity_search_with_score(sanitized_query, k=6)\n",
        "\n",
        "retrieved_docs = [doc for doc, _ in docs_with_scores]\n",
        "print(f\"   âœ… {len(retrieved_docs)} chunks retrieved\")\n",
        "\n",
        "# 2. Metrics\n",
        "scores_only = [score for _, score in docs_with_scores]\n",
        "retrieval_metrics = RetrievalMetrics.calculate_all(retrieved_docs, scores_only)\n",
        "print(f\"   â€¢ Diversity: {retrieval_metrics['diversity']:.3f}\")\n",
        "print(f\"   â€¢ Consistency: {retrieval_metrics['consistency']:.3f}\")\n",
        "\n",
        "# 3. Confidence\n",
        "retrieval_conf = ConfidenceCalculator.retrieval_confidence(docs_with_scores)\n",
        "relevance_conf = ConfidenceCalculator.context_relevance(sanitized_query, retrieved_docs, text_embed)\n",
        "print(f\"   â€¢ Retrieval conf: {retrieval_conf}\")\n",
        "print(f\"   â€¢ Relevance conf: {relevance_conf}\")\n",
        "\n",
        "# 4. Context\n",
        "def format_docs(docs):\n",
        "    parts = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        parts.append(f\"[Sezione {i}]\\n{doc.page_content.strip()}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)\n",
        "\n",
        "context_text = format_docs(retrieved_docs)\n",
        "\n",
        "# 5. RAG Chain (FIXED prompt)\n",
        "print(\"ðŸ”— Creazione RAG chain...\")\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "if detected_vehicle:\n",
        "    template = \"\"\"Sei un esperto tecnico specializzato nel manuale {vehicle}.\n",
        "\n",
        "ISTRUZIONI:\n",
        "1. Rispondi SOLO usando le informazioni nel CONTESTO\n",
        "2. Se l'informazione non Ã¨ nel contesto, dillo chiaramente\n",
        "3. Includi TUTTI i dettagli tecnici: procedure, valori, avvertenze\n",
        "4. Rispondi in italiano\n",
        "\n",
        "CONTESTO DAL MANUALE {vehicle}:\n",
        "{context}\n",
        "\n",
        "DOMANDA:\n",
        "{question}\n",
        "\n",
        "RISPOSTA TECNICA DETTAGLIATA:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\", \"vehicle\"])\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": RunnableLambda(lambda _: context_text),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"vehicle\": RunnableLambda(lambda _: detected_vehicle)\n",
        "        }\n",
        "        | prompt | llm | StrOutputParser()\n",
        "    )\n",
        "else:\n",
        "    template = \"\"\"Sei un esperto tecnico di manuali automotive.\n",
        "\n",
        "ISTRUZIONI:\n",
        "1. Rispondi usando le informazioni nel CONTESTO\n",
        "2. Se non trovi l'informazione, dillo\n",
        "3. Includi dettagli tecnici\n",
        "4. Rispondi in italiano\n",
        "\n",
        "CONTESTO:\n",
        "{context}\n",
        "\n",
        "DOMANDA:\n",
        "{question}\n",
        "\n",
        "RISPOSTA:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": RunnableLambda(lambda _: context_text),\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "        | prompt | llm | StrOutputParser()\n",
        "    )\n",
        "\n",
        "# 6. Generate\n",
        "print(\"ðŸ’¬ Generazione risposta...\")\n",
        "llm_start = time.time()\n",
        "response = chain.invoke(sanitized_query)\n",
        "llm_time = (time.time() - llm_start) * 1000\n",
        "print(f\"   âœ… Risposta generata ({llm_time:.1f}ms)\")\n",
        "\n",
        "# 7. Quality confidence\n",
        "quality_conf = ConfidenceCalculator.answer_quality(response)\n",
        "print(f\"   â€¢ Answer quality: {quality_conf}\")\n",
        "\n",
        "rag_time = (time.time() - start_time) * 1000\n",
        "\n",
        "confidence = ConfidenceCalculator.aggregate(retrieval_conf, relevance_conf, quality_conf)\n",
        "\n",
        "# ==========================================\n",
        "# DISPLAY RESPONSE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“– RISPOSTA\")\n",
        "if detected_vehicle:\n",
        "    print(f\"ðŸš— Manuale: {detected_vehicle}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n\" + response + \"\\n\")\n",
        "\n",
        "emoji = 'ðŸŸ¢' if confidence['label'] == 'HIGH' else ('ðŸŸ¡' if confidence['label'] == 'MEDIUM' else 'ðŸ”´')\n",
        "print(\"=\"*70)\n",
        "print(f\"{emoji} CONFIDENCE: {confidence['label']} ({confidence['score']})\")\n",
        "print(\"=\"*70)\n",
        "print(f\"   ðŸ” Retrieval: {confidence['breakdown']['retrieval']:.3f}\")\n",
        "print(f\"   ðŸŽ¯ Relevance: {confidence['breakdown']['relevance']:.3f}\")\n",
        "print(f\"   âœï¸ Quality: {confidence['breakdown']['quality']:.3f}\")\n",
        "\n",
        "# ==========================================\n",
        "# LLM JUDGE\n",
        "# ==========================================\n",
        "\n",
        "judge_data = None\n",
        "if ENABLE_LLM_JUDGE:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ¤– LLM JUDGE EVALUATION\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    judge = LLMJudge(llm)\n",
        "    judge_data = judge.evaluate_response(sanitized_query, context_text, response)\n",
        "\n",
        "    print(f\"   Faithfulness:  {judge_data['faithfulness']}/5  {'â­' * judge_data['faithfulness']}\")\n",
        "    print(f\"   Relevance:     {judge_data['relevance']}/5  {'â­' * judge_data['relevance']}\")\n",
        "    print(f\"   Completeness:  {judge_data['completeness']}/5  {'â­' * judge_data['completeness']}\")\n",
        "    print(f\"\\n   ðŸ† Average: {judge_data['average']:.2f}/5\")\n",
        "\n",
        "# ==========================================\n",
        "# IMAGE RETRIEVAL (WITH QUERY AUGMENTATION)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ–¼ï¸ IMAGE RETRIEVAL (con Query Augmentation)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "img_start = time.time()\n",
        "\n",
        "# âœ… NEW: Query augmentation per immagini\n",
        "if ENABLE_IMAGE_QUERY_AUGMENTATION:\n",
        "    print(\"ðŸ”§ Augmenting query per image retrieval...\")\n",
        "    image_query = augment_query_for_images(sanitized_query, llm)\n",
        "    print(f\"   Query originale: '{sanitized_query}'\")\n",
        "    print(f\"   Query augmented: '{image_query[:100]}...'\")\n",
        "else:\n",
        "    image_query = sanitized_query\n",
        "\n",
        "print(f\"\\n   ðŸš— Vehicle filter: {detected_vehicle}\")\n",
        "print(f\"   ðŸ“Š Stage 1: Top {STAGE1_TOP_K}\")\n",
        "print(f\"   ðŸŽ¯ Stage 2: Top {STAGE2_TOP_K}\")\n",
        "\n",
        "# Stage 1: Text similarity con query augmented\n",
        "if detected_vehicle:\n",
        "    img_candidates = img_db.similarity_search_with_score(\n",
        "        image_query, k=STAGE1_TOP_K*3, filter={\"manual\": detected_vehicle}\n",
        "    )[:STAGE1_TOP_K]\n",
        "else:\n",
        "    img_candidates = img_db.similarity_search_with_score(image_query, k=STAGE1_TOP_K)\n",
        "\n",
        "clip_available = sum(1 for doc, _ in img_candidates if doc.metadata.get('has_clip', False))\n",
        "print(f\"\\n   âœ… Stage 1: {len(img_candidates)} candidati ({clip_available} con CLIP)\")\n",
        "\n",
        "# Mostra se le immagini con \"TRIP\" nel content sono nei candidati\n",
        "trip_in_candidates = sum(1 for doc, _ in img_candidates if 'trip' in doc.page_content.lower())\n",
        "print(f\"   ðŸ“Š Candidati con 'TRIP' nel content: {trip_in_candidates}/{len(img_candidates)}\")\n",
        "\n",
        "# Stage 2: CLIP re-ranking\n",
        "inputs = clip_processor(text=[image_query], return_tensors=\"pt\", padding=True)\n",
        "inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    query_features = clip_model.get_text_features(**inputs)\n",
        "    query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "query_emb = query_features[0].cpu().numpy()\n",
        "\n",
        "reranked = []\n",
        "for doc, text_score in img_candidates:\n",
        "    img_id = doc.metadata.get('image_id')\n",
        "    has_clip = doc.metadata.get('has_clip', False)\n",
        "\n",
        "    if has_clip and img_id in clip_embeddings:\n",
        "        clip_emb = np.array(clip_embeddings[img_id])\n",
        "        clip_score = np.dot(query_emb, clip_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(clip_emb))\n",
        "        text_sim = 1 - text_score\n",
        "        combined = (CLIP_WEIGHT * clip_score) + ((1 - CLIP_WEIGHT) * text_sim)\n",
        "        reranked.append((doc, combined, True))\n",
        "    else:\n",
        "        reranked.append((doc, 1 - text_score, False))\n",
        "\n",
        "reranked.sort(key=lambda x: x[1], reverse=True)\n",
        "top_images = reranked[:STAGE2_TOP_K]\n",
        "\n",
        "print(f\"   âœ… Stage 2: Top {len(top_images)}\")\n",
        "\n",
        "# Quality filtering\n",
        "filtered_images = ImageQualityFilter.filter_results(top_images, quality_threshold)\n",
        "rejected = len(top_images) - len(filtered_images)\n",
        "print(f\"   âœ… Quality Filter: {len(filtered_images)}/{len(top_images)} ({rejected} rejected)\")\n",
        "\n",
        "img_time = (time.time() - img_start) * 1000\n",
        "\n",
        "# Display images\n",
        "print(f\"\\n   ðŸ† Top {len(filtered_images)} immagini:\")\n",
        "\n",
        "if filtered_images:\n",
        "    from PIL import Image as PILImage\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    num_imgs = min(len(filtered_images), 5)\n",
        "    fig, axes = plt.subplots(1, num_imgs, figsize=(4*num_imgs, 4))\n",
        "\n",
        "    if num_imgs == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, (doc, score, used_clip) in enumerate(filtered_images[:num_imgs]):\n",
        "        img_path = doc.metadata.get('image_path')\n",
        "\n",
        "        if img_path and os.path.exists(img_path):\n",
        "            try:\n",
        "                img = PILImage.open(img_path)\n",
        "                axes[idx].imshow(img)\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "                # Check se ha TRIP nel content\n",
        "                has_trip = 'âœ“TRIP' if 'trip' in doc.page_content.lower() else ''\n",
        "                method = \"CLIP\" if used_clip else \"Text\"\n",
        "                w, h = img.size\n",
        "                axes[idx].set_title(f\"#{idx+1} [{method}] {has_trip}\\n{w}x{h}px | {score:.3f}\", fontsize=9)\n",
        "            except:\n",
        "                axes[idx].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
        "                axes[idx].axis('off')\n",
        "        else:\n",
        "            axes[idx].text(0.5, 0.5, 'Not found', ha='center', va='center')\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n   ðŸ“ Dettagli:\")\n",
        "    for idx, (doc, score, used_clip) in enumerate(filtered_images[:num_imgs], 1):\n",
        "        caption = doc.metadata.get('caption', 'N/A')\n",
        "        has_trip = \"âœ… TRIP\" if 'trip' in doc.page_content.lower() else \"âŒ\"\n",
        "        method = \"CLIP\" if used_clip else \"Text\"\n",
        "\n",
        "        # Mostra snippet del context\n",
        "        context_snippet = doc.page_content[:100].replace('\\n', ' ')\n",
        "\n",
        "        print(f\"      {idx}. [{method}] {score:.3f} | {has_trip}\")\n",
        "        print(f\"         Context: '{context_snippet}...'\")\n",
        "else:\n",
        "    print(\"\\n   âš ï¸ Nessuna immagine trovata\")\n",
        "\n",
        "# Cleanup\n",
        "del query_features, query_emb, inputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ==========================================\n",
        "# SUMMARY\n",
        "# ==========================================\n",
        "\n",
        "total_time = (time.time() - start_time) * 1000\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "â±ï¸ Performance:\n",
        "   â€¢ Total: {total_time:.1f}ms\n",
        "   â€¢ RAG: {rag_time:.1f}ms\n",
        "   â€¢ LLM: {llm_time:.1f}ms\n",
        "   â€¢ Images: {img_time:.1f}ms\n",
        "\n",
        "ðŸš— Vehicle: {detected_vehicle}\n",
        "\n",
        "ðŸŽ¯ Confidence: {confidence['score']} ({confidence['label']})\n",
        "\"\"\")\n",
        "\n",
        "if judge_data:\n",
        "    print(f\"ðŸ¤– LLM Judge: {judge_data['average']:.2f}/5\")\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ–¼ï¸ Images:\n",
        "   â€¢ Query Augmentation: {'ON' if ENABLE_IMAGE_QUERY_AUGMENTATION else 'OFF'}\n",
        "   â€¢ Candidati con TRIP: {trip_in_candidates}/{len(img_candidates)}\n",
        "   â€¢ After filter: {len(filtered_images)}\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… TEST COMPLETATO!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiHMuLbVQErC"
      },
      "source": [
        "#Web App streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxBdRt59x5Vn"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BLOCCO 4: STREAMLIT APP - AutoMATE (v3.5 - ITALIAN MANUALS OPTIMIZED)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ðŸš€ BLOCCO 4: Streamlit App - AutoMATE (v3.5 - ITALIAN MANUALS OPTIMIZED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==========================================\n",
        "# VERIFICA PREREQUISITI\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ” Checking prerequisites...\")\n",
        "\n",
        "try:\n",
        "    _test_config = IMAGE_QUALITY_CONFIG\n",
        "    print(f\"âœ… BLOCCO 1 (Config)\")\n",
        "except NameError:\n",
        "    print(\"âŒ ERROR: BLOCCO 1 not executed!\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# LEGGI VALORI DA CONFIG\n",
        "# ==========================================\n",
        "\n",
        "DEFAULT_MIN_WIDTH = int(IMAGE_QUALITY_CONFIG['min_width'])\n",
        "DEFAULT_MIN_HEIGHT = int(IMAGE_QUALITY_CONFIG['min_height'])\n",
        "DEFAULT_MIN_SIZE_KB = float(IMAGE_QUALITY_CONFIG['min_size_kb'])\n",
        "DEFAULT_MAX_ASPECT_RATIO = float(IMAGE_QUALITY_CONFIG['max_aspect_ratio'])\n",
        "\n",
        "STAGE1_K = int(STAGE1_TOP_K)\n",
        "STAGE2_K = min(int(STAGE2_TOP_K), 10)\n",
        "CLIP_W = float(CLIP_WEIGHT)\n",
        "\n",
        "print(f\"âœ… Thresholds from BLOCCO 1:\")\n",
        "print(f\"   â€¢ Resolution: {DEFAULT_MIN_WIDTH}x{DEFAULT_MIN_HEIGHT}px\")\n",
        "print(f\"   â€¢ Size: {DEFAULT_MIN_SIZE_KB}KB\")\n",
        "print(f\"   â€¢ Stage 2 default: {STAGE2_K} (range 1-10)\")\n",
        "\n",
        "# ==========================================\n",
        "# INSTALLAZIONE DIPENDENZE\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ“¦ Installing dependencies...\")\n",
        "!pip install -q streamlit pyngrok\n",
        "print(\"âœ… Dependencies installed\")\n",
        "\n",
        "# ==========================================\n",
        "# GENERA APP.PY (FULLY EMBEDDED + CROSS-LANGUAGE)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ“ Generating app.py (fully embedded + cross-language optimized)...\")\n",
        "\n",
        "app_code = f'''\n",
        "# ==========================================\n",
        "# AutoMATE v3.5 - Italian Manuals Optimized\n",
        "# ==========================================\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==========================================\n",
        "# EMBEDDED CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "GOOGLE_API_KEY = \"{GOOGLE_API_KEY}\"\n",
        "\n",
        "CHROMA_TEXT_DIR = \"{CHROMA_TEXT_DIR}\"\n",
        "CHROMA_IMAGE_DIR = \"{CHROMA_IMAGE_DIR}\"\n",
        "CLIP_JSON_PATH = \"{CLIP_JSON_PATH}\"\n",
        "\n",
        "TEXT_EMBED_MODEL = \"{TEXT_EMBED_MODEL}\"\n",
        "CLIP_MODEL_NAME = \"{CLIP_MODEL_NAME}\"\n",
        "TEXT_COLLECTION_NAME = \"{TEXT_COLLECTION_NAME}\"\n",
        "IMAGE_COLLECTION_NAME = \"{IMAGE_COLLECTION_NAME}\"\n",
        "\n",
        "STAGE1_TOP_K = {STAGE1_K}\n",
        "STAGE2_TOP_K = {STAGE2_K}\n",
        "DEFAULT_CLIP_WEIGHT = {CLIP_W}\n",
        "\n",
        "DEFAULT_MIN_WIDTH = {DEFAULT_MIN_WIDTH}\n",
        "DEFAULT_MIN_HEIGHT = {DEFAULT_MIN_HEIGHT}\n",
        "DEFAULT_MIN_SIZE_KB = {DEFAULT_MIN_SIZE_KB}\n",
        "DEFAULT_MAX_ASPECT_RATIO = {DEFAULT_MAX_ASPECT_RATIO}\n",
        "\n",
        "EVAL_BASE = Path(\"/content/drive/MyDrive/OCR/evaluation\")\n",
        "QUERIES_LOG = EVAL_BASE / \"logs\" / \"queries.jsonl\"\n",
        "REPORT_PATH = EVAL_BASE / \"reports\" / \"automate_evaluation_report.md\"\n",
        "\n",
        "# ==========================================\n",
        "# IMPORTS\n",
        "# ==========================================\n",
        "\n",
        "try:\n",
        "    from langchain_chroma import Chroma\n",
        "except ImportError:\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, HarmCategory, HarmBlockThreshold\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "# ==========================================\n",
        "# PAGE CONFIG\n",
        "# ==========================================\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AutoMATE - Automotive Technical Assistant\",\n",
        "    page_icon=\"ðŸš—\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# SESSION STATE\n",
        "# ==========================================\n",
        "\n",
        "for key in ['resources_loaded', 'text_db', 'img_db', 'clip_embeddings',\n",
        "            'clip_model', 'clip_processor', 'llm', 'text_embed', 'query_history']:\n",
        "    if key not in st.session_state:\n",
        "        st.session_state[key] = None if key != 'resources_loaded' else False\n",
        "        if key == 'query_history':\n",
        "            st.session_state[key] = []\n",
        "\n",
        "# ==========================================\n",
        "# CSS\n",
        "# ==========================================\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {{\n",
        "        font-size: 2.5rem;\n",
        "        font-weight: bold;\n",
        "        background: linear-gradient(90deg, #1f77b4, #2ca02c);\n",
        "        -webkit-background-clip: text;\n",
        "        -webkit-text-fill-color: transparent;\n",
        "        text-align: center;\n",
        "        padding: 1rem 0;\n",
        "    }}\n",
        "    .subtitle {{\n",
        "        font-size: 0.9rem;\n",
        "        color: #666;\n",
        "        text-align: center;\n",
        "        font-style: italic;\n",
        "        margin-top: -15px;\n",
        "        margin-bottom: 20px;\n",
        "    }}\n",
        "    .confidence-high {{\n",
        "        background: linear-gradient(90deg, #4CAF50, #45a049);\n",
        "        color: white;\n",
        "        padding: 12px;\n",
        "        border-radius: 8px;\n",
        "        font-weight: bold;\n",
        "    }}\n",
        "    .confidence-medium {{\n",
        "        background: linear-gradient(90deg, #FF9800, #f57c00);\n",
        "        color: white;\n",
        "        padding: 12px;\n",
        "        border-radius: 8px;\n",
        "        font-weight: bold;\n",
        "    }}\n",
        "    .confidence-low {{\n",
        "        background: linear-gradient(90deg, #F44336, #d32f2f);\n",
        "        color: white;\n",
        "        padding: 12px;\n",
        "        border-radius: 8px;\n",
        "        font-weight: bold;\n",
        "    }}\n",
        "    .vehicle-badge {{\n",
        "        background: #e3f2fd;\n",
        "        color: #1976d2;\n",
        "        padding: 5px 15px;\n",
        "        border-radius: 20px;\n",
        "        font-weight: bold;\n",
        "        display: inline-block;\n",
        "    }}\n",
        "    .filter-disabled {{\n",
        "        background: #fff3e0;\n",
        "        border: 2px dashed #ff9800;\n",
        "        padding: 10px;\n",
        "        border-radius: 8px;\n",
        "        margin: 10px 0;\n",
        "    }}\n",
        "    .augment-enabled {{\n",
        "        background: #e8f5e9;\n",
        "        border: 2px solid #4CAF50;\n",
        "        padding: 10px;\n",
        "        border-radius: 8px;\n",
        "        margin: 10px 0;\n",
        "        font-size: 0.85em;\n",
        "    }}\n",
        "    .augment-disabled {{\n",
        "        background: #fafafa;\n",
        "        border: 1px dashed #ccc;\n",
        "        padding: 10px;\n",
        "        border-radius: 8px;\n",
        "        margin: 10px 0;\n",
        "    }}\n",
        "    .lang-badge {{\n",
        "        background: #f3e5f5;\n",
        "        color: #7b1fa2;\n",
        "        padding: 3px 10px;\n",
        "        border-radius: 12px;\n",
        "        font-size: 0.85em;\n",
        "        font-weight: bold;\n",
        "        display: inline-block;\n",
        "    }}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ==========================================\n",
        "# EMBEDDED CLASSES (from BLOCCO 2)\n",
        "# ==========================================\n",
        "\n",
        "class ConfidenceCalculator:\n",
        "    @staticmethod\n",
        "    def retrieval_confidence(docs_with_scores: List[Tuple]) -> float:\n",
        "        if not docs_with_scores:\n",
        "            return 0.0\n",
        "        scores = [1 - min(score, 1.0) for _, score in docs_with_scores]\n",
        "        avg = np.mean(scores)\n",
        "        consistency = 1 - np.std(scores) if len(scores) > 1 else 1.0\n",
        "        manuals = set(doc.metadata.get('manual', '') for doc, _ in docs_with_scores)\n",
        "        penalty = 0.95 if len(manuals) > 2 else 1.0\n",
        "        return round((avg * 0.6 + consistency * 0.4) * penalty, 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def context_relevance(query: str, chunks: List, embed_model) -> float:\n",
        "        if not chunks:\n",
        "            return 0.0\n",
        "        try:\n",
        "            query_emb = np.array(embed_model.embed_query(query))\n",
        "            relevances = []\n",
        "            for chunk in chunks:\n",
        "                chunk_emb = np.array(embed_model.embed_query(chunk.page_content[:500]))\n",
        "                sim = np.dot(query_emb, chunk_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb))\n",
        "                relevances.append(sim)\n",
        "            return round(np.mean(relevances), 3)\n",
        "        except:\n",
        "            return 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def answer_quality(response: str) -> float:\n",
        "        if not response or len(response.strip()) < 10:\n",
        "            return 0.0\n",
        "        score = 0.0\n",
        "        words = response.split()\n",
        "        length = len(words)\n",
        "        if 50 < length < 300:\n",
        "            score += 0.35\n",
        "        elif 30 < length <= 50:\n",
        "            score += 0.20\n",
        "        if re.search(r'\\\\d+\\\\s*(km|m|cm|mm|kg|g|bar|V|A|kW|rpm|l)', response):\n",
        "            score += 0.15\n",
        "        keywords = ['premere', 'ruotare', 'spegnere', 'accendere', 'verificare', 'controllare',\n",
        "                   'press', 'turn', 'switch', 'check', 'verify']\n",
        "        if any(kw in response.lower() for kw in keywords):\n",
        "            score += 0.10\n",
        "        sentences = [s for s in response.split('.') if len(s.strip()) > 10]\n",
        "        if len(sentences) >= 3:\n",
        "            score += 0.15\n",
        "        fallback = ['non riesco', 'non posso', 'non ho informazioni', 'cannot', 'unable']\n",
        "        if not any(phrase in response.lower() for phrase in fallback):\n",
        "            score += 0.10\n",
        "        return round(min(score, 1.0), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate(retrieval: float, relevance: float, quality: float) -> Dict:\n",
        "        final = retrieval * 0.40 + relevance * 0.35 + quality * 0.25\n",
        "        if final >= 0.75:\n",
        "            label, color = \"HIGH\", \"#4CAF50\"\n",
        "        elif final >= 0.55:\n",
        "            label, color = \"MEDIUM\", \"#FF9800\"\n",
        "        else:\n",
        "            label, color = \"LOW\", \"#F44336\"\n",
        "        return {{\n",
        "            'score': round(final, 3),\n",
        "            'label': label,\n",
        "            'color': color,\n",
        "            'breakdown': {{\n",
        "                'retrieval': round(retrieval, 3),\n",
        "                'relevance': round(relevance, 3),\n",
        "                'quality': round(quality, 3)\n",
        "            }}\n",
        "        }}\n",
        "\n",
        "\n",
        "class RetrievalMetrics:\n",
        "    @staticmethod\n",
        "    def diversity_score(retrieved_docs: List) -> float:\n",
        "        if not retrieved_docs:\n",
        "            return 0.0\n",
        "        manuals = [doc.metadata.get('manual', 'unknown') for doc in retrieved_docs]\n",
        "        unique_manuals = len(set(manuals))\n",
        "        return round(unique_manuals / len(manuals), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_similarity_variance(scores: List[float]) -> float:\n",
        "        if len(scores) < 2:\n",
        "            return 1.0\n",
        "        return round(1 - np.std(scores), 3)\n",
        "\n",
        "    @staticmethod\n",
        "    def average_chunk_score(scores: List[float]) -> float:\n",
        "        if not scores:\n",
        "            return 0.0\n",
        "        similarities = [1 - min(s, 1.0) for s in scores]\n",
        "        return round(np.mean(similarities), 3)\n",
        "\n",
        "\n",
        "class VehicleDetector:\n",
        "    VEHICLE_MAPPING = {{\n",
        "        'peugeot 208': 'PEUGEOT 208',\n",
        "        '208': 'PEUGEOT 208',\n",
        "        'peugeot': 'PEUGEOT 208',\n",
        "        'fiat 500': '500',\n",
        "        '500': '500',\n",
        "        'cinquecento': '500',\n",
        "        'panda': 'PANDA',\n",
        "        'fiat panda': 'PANDA',\n",
        "        'grande punto': 'GRANDE-PUNTO',\n",
        "        'punto': 'GRANDE-PUNTO',\n",
        "        'fiat punto': 'GRANDE-PUNTO',\n",
        "    }}\n",
        "\n",
        "    @classmethod\n",
        "    def detect(cls, query: str) -> Optional[str]:\n",
        "        query_lower = query.lower()\n",
        "        sorted_variants = sorted(cls.VEHICLE_MAPPING.keys(), key=len, reverse=True)\n",
        "        for variant in sorted_variants:\n",
        "            if variant in query_lower:\n",
        "                return cls.VEHICLE_MAPPING[variant]\n",
        "        return None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ImageQualityThreshold:\n",
        "    min_width: int = DEFAULT_MIN_WIDTH\n",
        "    min_height: int = DEFAULT_MIN_HEIGHT\n",
        "    min_size_kb: float = DEFAULT_MIN_SIZE_KB\n",
        "    max_aspect_ratio: float = DEFAULT_MAX_ASPECT_RATIO\n",
        "\n",
        "\n",
        "class ImageQualityFilter:\n",
        "    @staticmethod\n",
        "    def is_quality_image(img_path: str, threshold: ImageQualityThreshold) -> Tuple[bool, str]:\n",
        "        try:\n",
        "            if not os.path.exists(img_path):\n",
        "                return False, \"FILE_NOT_FOUND\"\n",
        "            size_kb = os.path.getsize(img_path) / 1024\n",
        "            if size_kb < threshold.min_size_kb:\n",
        "                return False, f\"TOO_SMALL ({{size_kb:.1f}}KB)\"\n",
        "            with PILImage.open(img_path) as img:\n",
        "                width, height = img.size\n",
        "                if width < threshold.min_width or height < threshold.min_height:\n",
        "                    return False, f\"LOW_RES ({{width}}x{{height}})\"\n",
        "                aspect = max(width, height) / min(width, height)\n",
        "                if aspect > threshold.max_aspect_ratio:\n",
        "                    return False, f\"BAD_RATIO ({{aspect:.1f}})\"\n",
        "            return True, \"OK\"\n",
        "        except:\n",
        "            return False, \"ERROR\"\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_results(img_results: List, threshold: ImageQualityThreshold, enabled: bool = True) -> List:\n",
        "        if not enabled:\n",
        "            return img_results\n",
        "        filtered = []\n",
        "        for result in img_results:\n",
        "            doc = result[0]\n",
        "            img_path = doc.metadata.get('image_path')\n",
        "            is_quality, _ = ImageQualityFilter.is_quality_image(img_path, threshold)\n",
        "            if is_quality:\n",
        "                filtered.append(result)\n",
        "        return filtered\n",
        "\n",
        "\n",
        "class PromptProtection:\n",
        "    @staticmethod\n",
        "    def sanitize_query(query: str) -> Tuple[str, bool]:\n",
        "        original = query\n",
        "        patterns = [\n",
        "            (r'<\\\\|[^>]*\\\\|>', ''),\n",
        "            (r'system\\\\s*:', '', re.IGNORECASE),\n",
        "            (r'ignore\\\\s+(previous|all|above)', '', re.IGNORECASE),\n",
        "        ]\n",
        "        cleaned = query\n",
        "        for pattern, replacement, *flags in patterns:\n",
        "            flag = flags[0] if flags else 0\n",
        "            cleaned = re.sub(pattern, replacement, cleaned, flags=flag)\n",
        "        return cleaned.strip(), (cleaned != original)\n",
        "\n",
        "\n",
        "class LLMJudge:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def evaluate_response(self, query: str, context: str, response: str) -> Dict:\n",
        "        if len(context) > 2000:\n",
        "            context = context[:2000] + \"...\"\n",
        "\n",
        "        prompt = f\"\"\"Evaluate this RAG response (scale 1-5):\n",
        "\n",
        "CONTEXT: {{context[:1500]}}\n",
        "QUESTION: {{query}}\n",
        "ANSWER: {{response}}\n",
        "\n",
        "Evaluate:\n",
        "1. FAITHFULNESS: 1-5\n",
        "2. RELEVANCE: 1-5\n",
        "3. COMPLETENESS: 1-5\n",
        "\n",
        "Respond ONLY with JSON:\n",
        "{{{{\"faithfulness\": N, \"relevance\": N, \"completeness\": N, \"reasoning\": \"max 80 char\"}}}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self.llm.invoke(prompt)\n",
        "            json_str = result.content if hasattr(result, 'content') else str(result)\n",
        "            json_str = re.sub(r'```json\\\\s*', '', json_str)\n",
        "            json_str = re.sub(r'```\\\\s*$', '', json_str)\n",
        "            scores = json.loads(json_str.strip())\n",
        "            scores['average'] = round((scores['faithfulness'] + scores['relevance'] + scores['completeness']) / 3, 2)\n",
        "            return scores\n",
        "        except Exception as e:\n",
        "            return {{'faithfulness': 0, 'relevance': 0, 'completeness': 0, 'average': 0, 'reasoning': str(e)[:50]}}\n",
        "\n",
        "\n",
        "def augment_query_for_images(query: str, llm_instance) -> Tuple[str, str]:\n",
        "    augment_prompt = f\"\"\"Given this automotive manual question:\n",
        "\"{{query}}\"\n",
        "\n",
        "Generate 5-8 keywords for finding relevant images. Include: components, positions, actions.\n",
        "Respond ONLY with keywords separated by spaces.\n",
        "Example: button dashboard display reset lever\"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm_instance.invoke(augment_prompt)\n",
        "        keywords = result.content if hasattr(result, 'content') else str(result)\n",
        "        keywords = keywords.strip().replace('\\\\n', ' ').replace(',', ' ')\n",
        "        keywords = re.sub(r'[^a-zA-Z\\\\s]', '', keywords)\n",
        "        keywords = ' '.join(keywords.split())\n",
        "        augmented = f\"{{query}} {{keywords}}\"\n",
        "        return augmented, keywords\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Augmentation failed: {{e}}\")\n",
        "        return query, \"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QueryLogEntry:\n",
        "    timestamp: str\n",
        "    query: str\n",
        "    response: str\n",
        "    vehicle: Optional[str]\n",
        "    confidence: Dict\n",
        "    llm_judge: Dict\n",
        "    retrieval_metrics: Dict\n",
        "    performance: Dict\n",
        "    clip_used: bool\n",
        "    injection_detected: bool\n",
        "    images_found: int\n",
        "    images_shown: int\n",
        "    language: str\n",
        "\n",
        "\n",
        "class EvaluationLogger:\n",
        "    def __init__(self):\n",
        "        self.queries_log = QUERIES_LOG\n",
        "        self.report_path = REPORT_PATH\n",
        "        self.queries_log.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def log_query(self, data: Dict):\n",
        "        entry = QueryLogEntry(\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "            query=data.get('query', ''),\n",
        "            response=data.get('response', ''),\n",
        "            vehicle=data.get('vehicle'),\n",
        "            confidence=data.get('confidence', {{}}),\n",
        "            llm_judge=data.get('llm_judge', {{}}),\n",
        "            retrieval_metrics=data.get('retrieval_metrics', {{}}),\n",
        "            performance=data.get('performance', {{}}),\n",
        "            clip_used=data.get('clip_used', False),\n",
        "            injection_detected=data.get('injection_detected', False),\n",
        "            images_found=data.get('images_found', 0),\n",
        "            images_shown=data.get('images_shown', 0),\n",
        "            language=data.get('language', 'unknown')\n",
        "        )\n",
        "\n",
        "        with open(self.queries_log, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\\\\n')\n",
        "\n",
        "        self.generate_markdown_report()\n",
        "\n",
        "    def load_all_queries(self) -> List[Dict]:\n",
        "        if not self.queries_log.exists():\n",
        "            return []\n",
        "        queries = []\n",
        "        with open(self.queries_log, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    queries.append(json.loads(line))\n",
        "                except:\n",
        "                    continue\n",
        "        return queries\n",
        "\n",
        "    def generate_markdown_report(self) -> str:\n",
        "        queries = self.load_all_queries()\n",
        "\n",
        "        if not queries:\n",
        "            report = \"# AutoMATE Evaluation Report\\\\n\\\\n**No data yet.**\"\n",
        "            with open(self.report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(report)\n",
        "            return str(self.report_path)\n",
        "\n",
        "        confidences = [q.get('confidence', {{}}).get('score', 0) for q in queries]\n",
        "        judge_scores = [q.get('llm_judge', {{}}).get('average', 0) for q in queries if q.get('llm_judge', {{}}).get('average', 0) > 0]\n",
        "\n",
        "        ret_div = [q.get('retrieval_metrics', {{}}).get('diversity', 0) for q in queries]\n",
        "        ret_con = [q.get('retrieval_metrics', {{}}).get('consistency', 0) for q in queries]\n",
        "        ret_sim = [q.get('retrieval_metrics', {{}}).get('avg_similarity', 0) for q in queries]\n",
        "\n",
        "        perf_tot = [q.get('performance', {{}}).get('total_time_ms', 0) for q in queries]\n",
        "        perf_rag = [q.get('performance', {{}}).get('rag_time_ms', 0) for q in queries]\n",
        "        perf_llm = [q.get('performance', {{}}).get('llm_time_ms', 0) for q in queries]\n",
        "        perf_img = [q.get('performance', {{}}).get('img_time_ms', 0) for q in queries]\n",
        "\n",
        "        last_10 = queries[-10:]\n",
        "\n",
        "        report = f\"\"\"# AutoMATE Evaluation Report\n",
        "**Automotive Multimodal Augmented Technical Expert**\n",
        "\n",
        "*Auto-generated: {{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}}*\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Executive Summary\n",
        "\n",
        "- **Total Queries**: {{len(queries)}}\n",
        "- **Period**: {{queries[0]['timestamp'][:10]}} to {{queries[-1]['timestamp'][:10]}}\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Aggregate Metrics\n",
        "\n",
        "### Confidence\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Mean | {{np.mean(confidences):.3f}} Â± {{np.std(confidences):.3f}} |\n",
        "| Range | [{{min(confidences):.3f}}, {{max(confidences):.3f}}] |\n",
        "\n",
        "### LLM Judge ({{len(judge_scores)}} samples)\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Mean | {{np.mean(judge_scores) if judge_scores else 0:.3f}}/5.0 Â± {{np.std(judge_scores) if judge_scores else 0:.3f}} |\n",
        "\n",
        "### Retrieval\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Diversity | {{np.mean(ret_div):.3f}} Â± {{np.std(ret_div):.3f}} |\n",
        "| Consistency | {{np.mean(ret_con):.3f}} Â± {{np.std(ret_con):.3f}} |\n",
        "| Avg Similarity | {{np.mean(ret_sim):.3f}} Â± {{np.std(ret_sim):.3f}} |\n",
        "\n",
        "### Performance\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Total Time | {{np.mean(perf_tot):.0f}}ms |\n",
        "| RAG Time | {{np.mean(perf_rag):.0f}}ms |\n",
        "| LLM Time | {{np.mean(perf_llm):.0f}}ms |\n",
        "| Image Time | {{np.mean(perf_img):.0f}}ms |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Last 10 Queries\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        for idx, q in enumerate(reversed(last_10), 1):\n",
        "            qtext = q.get('query', 'N/A')\n",
        "            vehicle = q.get('vehicle', 'Generic')\n",
        "            lang = q.get('language', '?')\n",
        "\n",
        "            conf = q.get('confidence', {{}})\n",
        "            conf_score = conf.get('score', 0)\n",
        "            conf_label = conf.get('label', 'N/A')\n",
        "            conf_bd = conf.get('breakdown', {{}})\n",
        "\n",
        "            judge = q.get('llm_judge', {{}})\n",
        "            ret = q.get('retrieval_metrics', {{}})\n",
        "            perf = q.get('performance', {{}})\n",
        "\n",
        "            report += f\"\"\"### Query #{{len(queries) - len(last_10) + idx}}: {{qtext[:70]}}{{\"...\" if len(qtext) > 70 else \"\"}}\n",
        "\n",
        "**Vehicle:** {{vehicle}} | **Language:** {{lang}}\n",
        "\n",
        "#### Confidence: {{conf_label}} ({{conf_score}})\n",
        "\n",
        "| Component | Score |\n",
        "|-----------|-------|\n",
        "| Retrieval | {{conf_bd.get('retrieval', 0)}} |\n",
        "| Relevance | {{conf_bd.get('relevance', 0)}} |\n",
        "| Quality | {{conf_bd.get('quality', 0)}} |\n",
        "\n",
        "#### Retrieval Metrics\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Diversity | {{ret.get('diversity', 0)}} |\n",
        "| Consistency | {{ret.get('consistency', 0)}} |\n",
        "| Avg Similarity | {{ret.get('avg_similarity', 0)}} |\n",
        "| Chunks | {{ret.get('num_chunks', 0)}} |\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            if judge.get('average', 0) > 0:\n",
        "                report += f\"\"\"#### LLM Judge: {{judge['average']}}/5\n",
        "\n",
        "| Dimension | Score |\n",
        "|-----------|-------|\n",
        "| Faithfulness | {{judge.get('faithfulness', 0)}}/5 |\n",
        "| Relevance | {{judge.get('relevance', 0)}}/5 |\n",
        "| Completeness | {{judge.get('completeness', 0)}}/5 |\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            report += f\"\"\"#### Performance\n",
        "\n",
        "Total: {{perf.get('total_time_ms', 0):.0f}}ms | RAG: {{perf.get('rag_time_ms', 0):.0f}}ms | LLM: {{perf.get('llm_time_ms', 0):.0f}}ms | Images: {{perf.get('img_time_ms', 0):.0f}}ms\n",
        "\n",
        "#### Images: {{q.get('images_shown', 0)}}/{{q.get('images_found', 0)}} shown\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        report += \"\\\\n*Auto-generated by AutoMATE v3.5*\\\\n\"\n",
        "\n",
        "        with open(self.report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        return str(self.report_path)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# RESOURCE LOADING\n",
        "# ==========================================\n",
        "\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    with st.spinner(\"Loading resources...\"):\n",
        "        text_embed = HuggingFaceEmbeddings(model_name=TEXT_EMBED_MODEL)\n",
        "\n",
        "        text_db = Chroma(\n",
        "            collection_name=TEXT_COLLECTION_NAME,\n",
        "            persist_directory=CHROMA_TEXT_DIR,\n",
        "            embedding_function=text_embed\n",
        "        )\n",
        "\n",
        "        img_db = Chroma(\n",
        "            collection_name=IMAGE_COLLECTION_NAME,\n",
        "            persist_directory=CHROMA_IMAGE_DIR,\n",
        "            embedding_function=text_embed\n",
        "        )\n",
        "\n",
        "        with open(CLIP_JSON_PATH, 'r') as f:\n",
        "            clip_embeddings = json.load(f)\n",
        "\n",
        "        clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to('cuda')\n",
        "        clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.0-flash-lite\",\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.1,\n",
        "            safety_settings={{\n",
        "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "            }}\n",
        "        )\n",
        "\n",
        "        return text_embed, text_db, img_db, clip_embeddings, clip_model, clip_processor, llm\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# MAIN APP\n",
        "# ==========================================\n",
        "\n",
        "st.markdown('<div class=\"main-header\">ðŸš— AutoMATE</div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"subtitle\">Automotive Multimodal Augmented Technical Expert</div>', unsafe_allow_html=True)\n",
        "st.markdown(\"**Vehicle-Aware â€¢ Cross-Language â€¢ Auto-Evaluation**\")\n",
        "\n",
        "# ==========================================\n",
        "# SIDEBAR\n",
        "# ==========================================\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"âš™ï¸ Settings\")\n",
        "\n",
        "    st.subheader(\"ðŸŒ Language\")\n",
        "    response_language = st.selectbox(\n",
        "        \"Response Language\",\n",
        "        [\"Italian\", \"English\", \"Auto (detect)\"],\n",
        "        index=1,\n",
        "        help=\"Manuals are in Italian. English queries will be auto-translated for retrieval.\"\n",
        "    )\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    st.subheader(\"ðŸ–¼ï¸ Images\")\n",
        "    enable_augmentation = st.checkbox(\"Query Augmentation\", True, help=\"Expands query with visual keywords\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    st.subheader(\"ðŸ” Quality Filter\")\n",
        "    enable_quality_filter = st.checkbox(\"Enable\", True, help=\"Filter out low-quality images\")\n",
        "\n",
        "    if enable_quality_filter:\n",
        "        min_width = st.slider(\"Min Width (px)\", 0, 200, DEFAULT_MIN_WIDTH, 10)\n",
        "        min_height = st.slider(\"Min Height (px)\", 0, 200, DEFAULT_MIN_HEIGHT, 10)\n",
        "        min_size_kb = st.slider(\"Min Size (KB)\", 0.0, 50.0, DEFAULT_MIN_SIZE_KB, 0.5)\n",
        "        max_aspect = st.slider(\"Max Ratio\", 2.0, 15.0, DEFAULT_MAX_ASPECT_RATIO, 0.5)\n",
        "    else:\n",
        "        min_width = 0\n",
        "        min_height = 0\n",
        "        min_size_kb = 0.0\n",
        "        max_aspect = 99.0\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    st.subheader(\"ðŸ“Š Retrieval\")\n",
        "    stage1_k = st.slider(\"Stage 1 (candidates)\", 10, 60, STAGE1_TOP_K, 5)\n",
        "    stage2_k = st.slider(\"Stage 2 (results)\", 1, 10, STAGE2_TOP_K, 1)\n",
        "    clip_weight = st.slider(\"CLIP Weight\", 0.0, 1.0, DEFAULT_CLIP_WEIGHT, 0.05)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    enable_judge = st.checkbox(\"LLM Judge\", False, help=\"Adds 3-5s per query\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    if st.session_state.resources_loaded:\n",
        "        st.success(\"âœ… Ready\")\n",
        "    else:\n",
        "        st.warning(\"â³ Loading...\")\n",
        "\n",
        "# ==========================================\n",
        "# LOAD RESOURCES\n",
        "# ==========================================\n",
        "\n",
        "if not st.session_state.resources_loaded:\n",
        "    try:\n",
        "        resources = load_resources()\n",
        "        (st.session_state.text_embed, st.session_state.text_db, st.session_state.img_db,\n",
        "         st.session_state.clip_embeddings, st.session_state.clip_model,\n",
        "         st.session_state.clip_processor, st.session_state.llm) = resources\n",
        "        st.session_state.resources_loaded = True\n",
        "        st.rerun()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error: {{e}}\")\n",
        "        st.stop()\n",
        "\n",
        "# ==========================================\n",
        "# QUERY INPUT\n",
        "# ==========================================\n",
        "\n",
        "query = st.text_input(\n",
        "    \"ðŸ” Enter your question:\",\n",
        "    placeholder=\"e.g., How does the TRIP button work? / Come funziona il pulsante TRIP?\"\n",
        ")\n",
        "\n",
        "submit = st.button(\"ðŸš€ Search\", type=\"primary\")\n",
        "\n",
        "# ==========================================\n",
        "# MAIN LOGIC\n",
        "# ==========================================\n",
        "\n",
        "if submit and query:\n",
        "    start_time = time.time()\n",
        "\n",
        "    sanitized_query, was_modified = PromptProtection.sanitize_query(query)\n",
        "    if was_modified:\n",
        "        st.warning(\"âš ï¸ Query sanitized for security\")\n",
        "\n",
        "    vehicle = VehicleDetector.detect(sanitized_query)\n",
        "\n",
        "    lang_badge = {{\"Italian\": \"ðŸ‡®ðŸ‡¹\", \"English\": \"ðŸ‡¬ðŸ‡§\", \"Auto (detect)\": \"ðŸŒ\"}}.get(response_language, \"ðŸŒ\")\n",
        "\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "    with col1:\n",
        "        if vehicle:\n",
        "            st.markdown(f'<span class=\"vehicle-badge\">{{vehicle}}</span>', unsafe_allow_html=True)\n",
        "    with col2:\n",
        "        st.markdown(f'<span class=\"lang-badge\">{{lang_badge}}</span>', unsafe_allow_html=True)\n",
        "\n",
        "    # ==========================================\n",
        "    # TEXT RAG (OPTIMIZED FOR ITALIAN MANUALS)\n",
        "    # ==========================================\n",
        "\n",
        "    rag_start = time.time()\n",
        "\n",
        "    with st.spinner(\"Processing RAG...\"):\n",
        "\n",
        "        # âœ… CROSS-LANGUAGE STRATEGY: Manuals are in Italian\n",
        "        retrieval_query = sanitized_query\n",
        "        needs_translation = False\n",
        "        translation_time = 0\n",
        "\n",
        "        # Detect if query needs translation\n",
        "        if response_language == \"English\":\n",
        "            needs_translation = True\n",
        "        elif response_language == \"Auto (detect)\":\n",
        "            # Simple language detection\n",
        "            en_indicators = ['how', 'what', 'does', 'work', 'button', 'display', 'when', 'where', 'the']\n",
        "            it_indicators = ['come', 'cosa', 'funziona', 'pulsante', 'display', 'quando', 'dove', 'il']\n",
        "\n",
        "            query_lower = sanitized_query.lower()\n",
        "            en_count = sum(1 for w in en_indicators if w in query_lower)\n",
        "            it_count = sum(1 for w in it_indicators if w in query_lower)\n",
        "\n",
        "            needs_translation = (en_count > it_count)\n",
        "\n",
        "        # Translate query for retrieval if needed\n",
        "        if needs_translation:\n",
        "            translation_start = time.time()\n",
        "\n",
        "            translate_prompt = f\"\"\"Translate this automotive technical question to Italian.\n",
        "Keep technical terms precise and maintain context.\n",
        "\n",
        "English: {{sanitized_query}}\n",
        "\n",
        "Italian translation (ONLY the translation, no explanations):\"\"\"\n",
        "\n",
        "            try:\n",
        "                translation_result = st.session_state.llm.invoke(translate_prompt)\n",
        "                retrieval_query = translation_result.content if hasattr(translation_result, 'content') else str(translation_result)\n",
        "                retrieval_query = retrieval_query.strip()\n",
        "\n",
        "                translation_time = (time.time() - translation_start) * 1000\n",
        "\n",
        "                # Show translation\n",
        "                with st.expander(\"ðŸ”„ Query Translation (for retrieval)\", expanded=False):\n",
        "                    st.caption(\"Manuals are in Italian. Query translated for better retrieval accuracy.\")\n",
        "                    st.write(f\"**Original:** {{sanitized_query}}\")\n",
        "                    st.write(f\"**Translated:** {{retrieval_query}}\")\n",
        "                    st.caption(f\"â±ï¸ Translation: {{translation_time:.0f}}ms\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.warning(f\"âš ï¸ Translation failed: {{e}}. Using original query (accuracy may be reduced).\")\n",
        "                retrieval_query = sanitized_query\n",
        "\n",
        "        # Retrieval with (possibly translated) query\n",
        "        retrieval_start = time.time()\n",
        "\n",
        "        if vehicle:\n",
        "            docs_with_scores = st.session_state.text_db.similarity_search_with_score(\n",
        "                retrieval_query, k=18, filter={{\"manual\": vehicle}}\n",
        "            )[:6]\n",
        "        else:\n",
        "            docs_with_scores = st.session_state.text_db.similarity_search_with_score(retrieval_query, k=6)\n",
        "\n",
        "        retrieved_docs = [doc for doc, _ in docs_with_scores]\n",
        "\n",
        "        retrieval_elapsed = (time.time() - retrieval_start) * 1000\n",
        "\n",
        "        # Retrieval quality check\n",
        "        if len(docs_with_scores) > 0:\n",
        "            avg_similarity = np.mean([1 - score for _, score in docs_with_scores])\n",
        "\n",
        "            with st.expander(\"ðŸ“Š Retrieval Quality\", expanded=False):\n",
        "                st.write(f\"**Chunks retrieved:** {{len(retrieved_docs)}}\")\n",
        "                st.write(f\"**Avg similarity:** {{avg_similarity:.3f}}\")\n",
        "\n",
        "                if avg_similarity < 0.5:\n",
        "                    st.warning(\"âš ï¸ Low retrieval quality. Results may be less accurate.\")\n",
        "                elif avg_similarity > 0.75:\n",
        "                    st.success(\"âœ… High retrieval quality!\")\n",
        "\n",
        "                st.write(f\"**Retrieval time:** {{retrieval_elapsed:.0f}}ms\")\n",
        "\n",
        "                # Preview top 3 chunks\n",
        "                st.caption(\"**Top 3 chunks:**\")\n",
        "                for i, (doc, score) in enumerate(docs_with_scores[:3], 1):\n",
        "                    sim = 1 - score\n",
        "                    snippet = doc.page_content[:80].replace('\\\\n', ' ')\n",
        "                    st.text(f\"{{i}}. [{{sim:.3f}}] {{snippet}}...\")\n",
        "\n",
        "        context_parts = [f\"[Section {{i+1}}]\\\\n{{doc.page_content.strip()}}\" for i, doc in enumerate(retrieved_docs)]\n",
        "        context_text = \"\\\\n\\\\n---\\\\n\\\\n\".join(context_parts)\n",
        "\n",
        "        # âœ… CROSS-LANGUAGE PROMPTS\n",
        "        if response_language == \"English\":\n",
        "            template = \"\"\"You are an expert automotive technician.\n",
        "\n",
        "IMPORTANT: The manual content below is in ITALIAN, but you must answer in ENGLISH.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Carefully read and understand the Italian technical documentation\n",
        "2. Answer ONLY using information from the provided context\n",
        "3. Translate technical procedures and specifications accurately to English\n",
        "4. If information is not in the context, clearly state: \"This information is not available in the manual\"\n",
        "5. Maintain technical precision and include all relevant details (values, warnings, procedures)\n",
        "\n",
        "CONTEXT (Italian manual):\n",
        "{{context}}\n",
        "\n",
        "USER QUESTION (English):\n",
        "{{question}}\n",
        "\n",
        "TECHNICAL ANSWER (English):\"\"\"\n",
        "\n",
        "        elif response_language == \"Auto (detect)\":\n",
        "            template = \"\"\"You are an expert automotive technician.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer ONLY using information from the CONTEXT\n",
        "2. If information is not in the context, clearly state so\n",
        "3. Include ALL technical details: procedures, values, warnings\n",
        "4. Respond in the SAME LANGUAGE as the question (Italian or English)\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "QUESTION:\n",
        "{{question}}\n",
        "\n",
        "TECHNICAL ANSWER:\"\"\"\n",
        "\n",
        "        else:  # Italian\n",
        "            template = \"\"\"Sei un tecnico esperto automotive.\n",
        "\n",
        "ISTRUZIONI:\n",
        "1. Rispondi SOLO usando informazioni dal CONTESTO fornito\n",
        "2. Se l'informazione non Ã¨ presente nel contesto, dillo chiaramente\n",
        "3. Includi TUTTI i dettagli tecnici: procedure, valori, avvertenze\n",
        "4. Rispondi in italiano\n",
        "\n",
        "CONTESTO:\n",
        "{{context}}\n",
        "\n",
        "DOMANDA:\n",
        "{{question}}\n",
        "\n",
        "RISPOSTA TECNICA:\"\"\"\n",
        "\n",
        "        prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        chain = (\n",
        "            {{\n",
        "                \"context\": RunnableLambda(lambda _: context_text),\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }}\n",
        "            | prompt\n",
        "            | st.session_state.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        # Generate with ORIGINAL query (not translated)\n",
        "        llm_start = time.time()\n",
        "        response = chain.invoke(sanitized_query)\n",
        "        llm_time = (time.time() - llm_start) * 1000\n",
        "\n",
        "        # Confidence metrics\n",
        "        ret_conf = ConfidenceCalculator.retrieval_confidence(docs_with_scores)\n",
        "        rel_conf = ConfidenceCalculator.context_relevance(sanitized_query, retrieved_docs, st.session_state.text_embed)\n",
        "        qual_conf = ConfidenceCalculator.answer_quality(response)\n",
        "        confidence = ConfidenceCalculator.aggregate(ret_conf, rel_conf, qual_conf)\n",
        "\n",
        "    rag_time = (time.time() - rag_start) * 1000\n",
        "\n",
        "    # ==========================================\n",
        "    # DISPLAY RESPONSE\n",
        "    # ==========================================\n",
        "\n",
        "    st.markdown(\"### ðŸ“– Answer\")\n",
        "    st.write(response)\n",
        "\n",
        "    conf_class = f\"confidence-{{confidence['label'].lower()}}\"\n",
        "    st.markdown(f'<div class=\"{{conf_class}}\">ðŸŽ¯ {{confidence[\"label\"]}} ({{confidence[\"score\"]}})</div>', unsafe_allow_html=True)\n",
        "\n",
        "    c1, c2, c3 = st.columns(3)\n",
        "    with c1:\n",
        "        st.metric(\"ðŸ” Retrieval\", f\"{{confidence['breakdown']['retrieval']}}\")\n",
        "    with c2:\n",
        "        st.metric(\"ðŸŽ¯ Relevance\", f\"{{confidence['breakdown']['relevance']}}\")\n",
        "    with c3:\n",
        "        st.metric(\"âœï¸ Quality\", f\"{{confidence['breakdown']['quality']}}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # LLM JUDGE\n",
        "    # ==========================================\n",
        "\n",
        "    judge_data = {{}}\n",
        "    if enable_judge:\n",
        "        with st.spinner(\"ðŸ¤– LLM Judge evaluation...\"):\n",
        "            judge = LLMJudge(st.session_state.llm)\n",
        "            judge_data = judge.evaluate_response(sanitized_query, context_text, response)\n",
        "\n",
        "        st.markdown(\"### ðŸ¤– LLM Judge\")\n",
        "        j1, j2, j3 = st.columns(3)\n",
        "        with j1:\n",
        "            st.metric(\"Faithfulness\", f\"{{judge_data.get('faithfulness', 0)}}/5\")\n",
        "        with j2:\n",
        "            st.metric(\"Relevance\", f\"{{judge_data.get('relevance', 0)}}/5\")\n",
        "        with j3:\n",
        "            st.metric(\"Completeness\", f\"{{judge_data.get('completeness', 0)}}/5\")\n",
        "\n",
        "        st.caption(f\"Average: {{judge_data.get('average', 0):.2f}}/5 | {{judge_data.get('reasoning', 'N/A')}}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # IMAGE RETRIEVAL\n",
        "    # ==========================================\n",
        "\n",
        "    img_start = time.time()\n",
        "\n",
        "    with st.spinner(\"ðŸ–¼ï¸ Searching images...\"):\n",
        "        augmented_keywords = \"\"\n",
        "        if enable_augmentation:\n",
        "            image_query, augmented_keywords = augment_query_for_images(sanitized_query, st.session_state.llm)\n",
        "        else:\n",
        "            image_query = sanitized_query\n",
        "\n",
        "        # Stage 1: Text similarity\n",
        "        if vehicle:\n",
        "            img_candidates = st.session_state.img_db.similarity_search_with_score(\n",
        "                image_query, k=stage1_k*3, filter={{\"manual\": vehicle}}\n",
        "            )[:stage1_k]\n",
        "        else:\n",
        "            img_candidates = st.session_state.img_db.similarity_search_with_score(image_query, k=stage1_k)\n",
        "\n",
        "        # Stage 2: CLIP re-ranking\n",
        "        inputs = st.session_state.clip_processor(text=[image_query], return_tensors=\"pt\", padding=True)\n",
        "        inputs = {{k: v.to('cuda') for k, v in inputs.items()}}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            query_features = st.session_state.clip_model.get_text_features(**inputs)\n",
        "            query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        query_emb = query_features[0].cpu().numpy()\n",
        "\n",
        "        reranked = []\n",
        "        for doc, text_score in img_candidates:\n",
        "            img_id = doc.metadata.get('image_id')\n",
        "            has_clip = doc.metadata.get('has_clip', False)\n",
        "\n",
        "            if has_clip and img_id in st.session_state.clip_embeddings:\n",
        "                clip_emb = np.array(st.session_state.clip_embeddings[img_id])\n",
        "                clip_score = np.dot(query_emb, clip_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(clip_emb))\n",
        "                text_sim = 1 - text_score\n",
        "                combined = (clip_weight * clip_score) + ((1 - clip_weight) * text_sim)\n",
        "                reranked.append((doc, combined, True))\n",
        "            else:\n",
        "                reranked.append((doc, 1 - text_score, False))\n",
        "\n",
        "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_imgs = reranked[:stage2_k]\n",
        "\n",
        "        # Quality filter\n",
        "        quality_threshold = ImageQualityThreshold(min_width, min_height, min_size_kb, max_aspect)\n",
        "        filtered_imgs = ImageQualityFilter.filter_results(top_imgs, quality_threshold, enable_quality_filter)\n",
        "\n",
        "        del query_features, query_emb, inputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    img_time = (time.time() - img_start) * 1000\n",
        "\n",
        "    # ==========================================\n",
        "    # DISPLAY IMAGES\n",
        "    # ==========================================\n",
        "\n",
        "    st.markdown(\"### ðŸ–¼ï¸ Relevant Images\")\n",
        "\n",
        "    if augmented_keywords and enable_augmentation:\n",
        "        with st.expander(\"ðŸ”§ Augmented Keywords\", expanded=False):\n",
        "            st.code(augmented_keywords)\n",
        "\n",
        "    rejected = len(top_imgs) - len(filtered_imgs)\n",
        "    st.caption(f\"Showing {{len(filtered_imgs)}}/{{stage2_k}} images ({{rejected}} filtered by quality)\")\n",
        "\n",
        "    if filtered_imgs:\n",
        "        cols = st.columns(min(len(filtered_imgs), 5))\n",
        "        for idx, (doc, score, used_clip) in enumerate(filtered_imgs):\n",
        "            with cols[idx % 5]:\n",
        "                img_path = doc.metadata.get('image_path')\n",
        "                if img_path and os.path.exists(img_path):\n",
        "                    try:\n",
        "                        img = PILImage.open(img_path)\n",
        "                        w, h = img.size\n",
        "                        method = \"CLIP\" if used_clip else \"Text\"\n",
        "                        st.image(img, caption=f\"#{{idx+1}} [{{method}}]\\\\n{{score:.3f}} | {{w}}x{{h}}px\", use_container_width=True)\n",
        "                    except:\n",
        "                        st.warning(\"Error loading image\")\n",
        "    else:\n",
        "        if enable_quality_filter:\n",
        "            st.info(\"No images passed quality filter. Try disabling it or adjusting thresholds.\")\n",
        "        else:\n",
        "            st.info(\"No images found for this query.\")\n",
        "\n",
        "    # ==========================================\n",
        "    # PERFORMANCE SUMMARY\n",
        "    # ==========================================\n",
        "\n",
        "    total_time = (time.time() - start_time) * 1000\n",
        "\n",
        "    st.caption(f\"â±ï¸ Total: {{total_time:.0f}}ms (RAG: {{rag_time:.0f}}ms | LLM: {{llm_time:.0f}}ms | Images: {{img_time:.0f}}ms)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # EVALUATION LOGGING\n",
        "    # ==========================================\n",
        "\n",
        "    try:\n",
        "        eval_logger = EvaluationLogger()\n",
        "\n",
        "        scores_only = [score for _, score in docs_with_scores]\n",
        "        retrieval_metrics = {{\n",
        "            'diversity': RetrievalMetrics.diversity_score(retrieved_docs),\n",
        "            'consistency': RetrievalMetrics.chunk_similarity_variance(scores_only),\n",
        "            'avg_similarity': RetrievalMetrics.average_chunk_score(scores_only),\n",
        "            'num_chunks': len(retrieved_docs)\n",
        "        }}\n",
        "\n",
        "        eval_logger.log_query({{\n",
        "            'query': query,\n",
        "            'response': response,\n",
        "            'vehicle': vehicle,\n",
        "            'confidence': confidence,\n",
        "            'llm_judge': judge_data,\n",
        "            'retrieval_metrics': retrieval_metrics,\n",
        "            'performance': {{\n",
        "                'total_time_ms': total_time,\n",
        "                'rag_time_ms': rag_time,\n",
        "                'llm_time_ms': llm_time,\n",
        "                'img_time_ms': img_time\n",
        "            }},\n",
        "            'clip_used': any(used_clip for _, _, used_clip in filtered_imgs),\n",
        "            'injection_detected': was_modified,\n",
        "            'images_found': len(top_imgs),\n",
        "            'images_shown': len(filtered_imgs),\n",
        "            'language': response_language\n",
        "        }})\n",
        "\n",
        "        st.success(f\"âœ… Evaluation report updated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.warning(f\"âš ï¸ Evaluation logging error: {{e}}\")\n",
        "\n",
        "    # History\n",
        "    st.session_state.query_history.append({{\n",
        "        'query': query[:50],\n",
        "        'vehicle': vehicle,\n",
        "        'confidence': confidence['score'],\n",
        "        'images': len(filtered_imgs),\n",
        "        'language': response_language\n",
        "    }})\n",
        "\n",
        "# ==========================================\n",
        "# FOOTER & HISTORY\n",
        "# ==========================================\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"AutoMATE v3.5 | Automotive Multimodal Augmented Technical Expert â€¢ Cross-Language Optimized\")\n",
        "\n",
        "if st.session_state.query_history:\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(\"ðŸ“œ Recent Queries\")\n",
        "        for idx, entry in enumerate(reversed(st.session_state.query_history[-5:]), 1):\n",
        "            lang_icon = {{\"Italian\": \"ðŸ‡®ðŸ‡¹\", \"English\": \"ðŸ‡¬ðŸ‡§\", \"Auto (detect)\": \"ðŸŒ\"}}.get(entry.get('language', ''), '')\n",
        "            st.caption(f\"{{idx}}. {{lang_icon}} {{entry['query']}}... ({{entry['confidence']:.2f}})\")\n",
        "'''\n",
        "\n",
        "# ==========================================\n",
        "# SAVE APP\n",
        "# ==========================================\n",
        "\n",
        "app_path = \"/content/app.py\"\n",
        "\n",
        "with open(app_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(f\"âœ… app.py generated: {app_path}\")\n",
        "print(f\"   â€¢ Size: {os.path.getsize(app_path) / 1024:.1f}KB\")\n",
        "print(f\"   â€¢ Features: Cross-language support + Query translation\")\n",
        "\n",
        "# ==========================================\n",
        "# START STREAMLIT + NGROK\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸš€ STARTING STREAMLIT + NGROK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "import subprocess\n",
        "import threading\n",
        "\n",
        "try:\n",
        "    ngrok_token = getpass.getpass(\"ðŸ”‘ Ngrok Auth Token: \")\n",
        "    conf.get_default().auth_token = ngrok_token\n",
        "\n",
        "    print(\"\\nðŸ”„ Starting Streamlit...\")\n",
        "\n",
        "    !pkill -9 streamlit 2>/dev/null\n",
        "\n",
        "    def run_streamlit():\n",
        "        subprocess.run([\n",
        "            \"streamlit\", \"run\", app_path,\n",
        "            \"--server.port=8501\",\n",
        "            \"--server.headless=true\",\n",
        "            \"--server.fileWatcherType=none\",\n",
        "            \"--browser.gatherUsageStats=false\"\n",
        "        ])\n",
        "\n",
        "    streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "    streamlit_thread.start()\n",
        "\n",
        "    import time\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"ðŸŒ Creating ngrok tunnel...\")\n",
        "    public_url = ngrok.connect(8501)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸŽ‰ AutoMATE IS ONLINE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nðŸ”— URL: {public_url}\")\n",
        "    print(f\"\"\"\n",
        "âœ… AutoMATE v3.5 - Italian Manuals Optimized\n",
        "\n",
        "ðŸŒ CROSS-LANGUAGE FEATURES:\n",
        "   â€¢ English queries â†’ Auto-translated to Italian for retrieval\n",
        "   â€¢ Italian context â†’ Accurate English responses\n",
        "   â€¢ Auto-detect mode for mixed language usage\n",
        "   â€¢ Translation transparency (see expander for details)\n",
        "\n",
        "ðŸ“Š EVALUATION:\n",
        "   â€¢ Auto-updating report after each query\n",
        "   â€¢ Retrieval quality monitoring\n",
        "   â€¢ Full metrics tracking (confidence, LLM judge, performance)\n",
        "   â€¢ Report: /content/drive/MyDrive/OCR/evaluation/reports/automate_evaluation_report.md\n",
        "\n",
        "ðŸ’¡ USAGE:\n",
        "   â€¢ Italian query: \"Come funziona il pulsante TRIP?\"\n",
        "     â†’ Direct retrieval, Italian response\n",
        "\n",
        "   â€¢ English query: \"How does the TRIP button work?\"\n",
        "     â†’ Auto-translated to Italian for retrieval\n",
        "     â†’ Italian context used to generate English response\n",
        "\n",
        "   â€¢ Auto mode: Detects language and responds accordingly\n",
        "\n",
        "ðŸ’¾ App stays online while notebook runs\n",
        "\"\"\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error: {e}\")\n",
        "    print(\"\\nðŸ”§ Troubleshooting:\")\n",
        "    print(\"   1. Get Ngrok token: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"   2. Restart runtime and try again\")\n",
        "\n",
        "# NGROK TOKEN\n",
        "#NGROK_AUTH_TOKEN = \"SAVE_HERE_YOUR_NGROK_TOKEN\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pp6wKN3lPfCb",
        "Dro9weG2Pn0Z",
        "6f4VKObvPuGM",
        "yGYT_jWtPyjp",
        "ljvjGbBpP5R2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
